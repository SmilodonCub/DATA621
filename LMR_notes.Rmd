---
title: 'Linear Models with `R`'
subtitle: 'by Julian Faraway'
author: 'notes by Bonnie Cooper'
output:
  rmdformats::downcute
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following are notes from readings in ['A Modern Approach to Regression with `R`'](https://www.routledge.com/Linear-Models-with-R/Faraway/p/book/9781439887332) by Julian Faraway for the course DATA621, 'Business Analystics and Data Mining' as part of the [Masters of Science in Data Science program at CUNY SPS](https://sps.cuny.edu/academics/graduate/master-science-data-science-ms).

`R` libraries used:
```{r message=FALSE}
library( faraway )
library( dplyr )
library( ggplot2 )
library( gridExtra )
library( tidyverse )
library( HistData )
library( broom )
library( gclus )
library( asbio )
library( retistruct )
library( Matching )
```

## Introduction
*"The formulation of a problem is often more essential than its solution"*

**Formulate the Problem Correctly**  

* Understand the Physical Background
* Understand the Objective
* Make sure you know what the client wants
* Put the Problem into Statistical Terms
  + observational or experimental data?
  + Is there Nonresponse?
  + Are there Missing Values?
  + How are the Data Coded?
  + What are the Units of Measurement?
  + Beware of data entry errors and other forms of data corruption
  
  
### Initial Data Analysis  
Critical to do an initial exploration to get the feel for the data. summary statistics. basic visualizations. inspect quality of the data...

look at some practice data:
```{r}
data( pima )
head( pima )
```
```{r}
summary( pima )
```

Not all missing values are `NA`. the minimum for `diastolic` is $0$. However, that's not a realistic value for a living person. Therefore, it is more likely that this is a missing value. Look closer at `diastolic`:
```{r}
sort( pima$diastolic )[1:100]
```
clean these values up for `diastolic` and the other similar numeric variables
```{r}
pima_NA <- pima %>%
  mutate( diastolic =  na_if( diastolic, 0 ),
             glucose = na_if( glucose, 0 ),
             triceps = na_if( triceps, 0 ),
             insulin = na_if( insulin, 0 ),
             bmi = na_if( bmi, 0 ) ) 
glimpse( pima_NA )
```
change the `test` feature to a categorical with descriptive labels:
```{r}
pima_NA$test <- factor( pima_NA$test )
levels( pima_NA$test ) <- c( 'negative', 'positive' )
summary( pima_NA$test )
```
Do some basic visualization of some features:
```{r}
par(mfrow=c(1,3))
pimahist <- hist( pima_NA$diastolic, xlab='Diastolic',main='' )
pimadens <- plot( density( pima_NA$diastolic, na.rm=TRUE ), main="" )
pimasort <- plot( sort( pima_NA$diastolic ), ylab = 'Sorted Diastolic' )
```

Now try again with ggplot:
```{r}
pimahist <- ggplot( pima_NA, aes( x = diastolic ) ) +
  geom_histogram()
pimadens <- ggplot( pima_NA, aes( x = diastolic ) ) +
  geom_density()
sdiastolic <- sort( pima_NA$diastolic )
pima_sort <- data.frame( 'sorted' = sdiastolic )
pimasort <- ggplot( pima_sort, aes( y = sorted, x = 1:length( sdiastolic ) ) ) +
  geom_point()

grid.arrange( pimahist, pimadens, pimasort, ncol = 3 )
```

Visualize some bivariate data
```{r}
p1 <- ggplot( pima_NA, aes( x = diastolic, y = diabetes ) ) +
  geom_point()

p2 <- ggplot( pima_NA, aes( x = test, y = diabetes ) ) +
  geom_boxplot()

grid.arrange( p1, p2, ncol = 2 )
```

`ggplot2` is a handy library that is more flexible when visualizing complex dataframes. For instance: controlling color/shape/fill by another variables, or creating faceted plots:
```{r}
#bisambiguating a factor variable:
p1 <- ggplot( pima, aes( x = diastolic, y = diabetes, color = factor( test ) ) ) +
  geom_point() +
  theme( legend.position = 'top',
         legend.direction = 'horizontal' )
p2 <- ggplot( pima, aes( x = diastolic, y = diabetes ) ) +
  geom_point( size = 1 ) +
  facet_grid( ~ factor( test ) )

grid.arrange(p1, p2, ncol=2,top="Disambiguating a factor variable")
```
**On Visualizations**: Good graphics are vital in data analysis. They help you avoid mistakes and suggest the form of the modeling to come. They are also important in communicating your analysis to others. Many in your audience or readership will focus on the graphs. This is your best opportunity to get your message over clearly and without misunderstanding. In some cases, the graphics can be so convincing that the formal analysis becomes just a confirmation of what has already been seen

### When to use Linear Modeling
**Linear Modeling** is used for explaining or modeling the relationship between a response/outcome/output variable and one or more predictor/input/explanatory variable(s).  
**Simple Regression**: modeling an outcome variable with just 1 explanatory variable.  
**Mulitple/Multivariate Regression**: modeling an outcome variable with more than 1 explanatory varaible.  

**Regression Objectives**  

* Prediction of future or unseen responses given specified values of the predictors
* Assessment of an effect, or relationship between, explanatory variables and the response

### History of Regression
describing the [libration of the moon](https://www.google.com/books/edition/The_History_of_Statistics/M7yvkERHIIMC?hl=en&gbpv=1&dq=stigler+1986&pg=PA1&printsec=frontcover):
```{r}
data( manilius )
glimpse( manilius )
```
The data are divided into three groups based on similarity. Next, compute the sum of the three coefficients by group.

```{r}
moon3 <- manilius %>%
  group_by( group ) %>%
  summarise( arc_sum = sum( arc ),
             sin_sum = sum( sinang ),
             cos_sum = sum( cosang ) )
moon3
```
The result are 3 linear equations with three unknowns each to solve
```{r}
solve( cbind( 9, moon3$sin_sum, moon3$cos_sum ), moon3$arc_sum )
```
Observe how similar the results are if we fit a linear regression to the original data:
```{r}
mod <- lm( arc ~ sinang + cosang, manilius )
summary( mod )
```
The word regression derives from a term of Sir Francis Galton's: *regression to mediocrity*  
**Important**: regression to mediocrity refers to a particular statistical finding and is quite a different concept to that of regression.  
Sir Francis Galton found that outcomes have a tendency towards the mean of the data using the heights of parents and their offspring. That is, tall parents are more likely to have a child shorter than they are and short parents are more likely to have a child taller than they are.  
Here we explore this data:  
```{r}
data( GaltonFamilies )
ggplot( GaltonFamilies, aes( x = midparentHeight,
                             y = childHeight ) ) +
  geom_point( size = 3 )
```
find the linear regression line the best fits the distribution
```{r}
mod <- lm( childHeight ~ midparentHeight, GaltonFamilies )
coef( mod )
```
```{r}
ggplot(GaltonFamilies, aes(x = midparentHeight, y = childHeight)) + 
  geom_point() +
  geom_smooth( method = "lm" )
```
Let's add the line that describes a relation where parents have children the same height as they are...
```{r}
ggplot(GaltonFamilies, aes(x = midparentHeight, y = childHeight)) + 
  geom_point() +
  geom_smooth( method = "lm" ) +
  geom_line( aes( y = midparentHeight ), color = 'red' )
```

Now suppose children height is fully correlated to thier parents. We can use the following equation:
$$\frac{y-\bar{y}}{SD_y} = r\frac{x-\bar{x}}{SD_x}$$
where the correlation, $r = 1$ to find the coefficients of a line describing this:
```{r}
beta <- with( GaltonFamilies,
                sd( childHeight )/ sd( midparentHeight ) )
alpha <- with( GaltonFamilies,
               mean( childHeight ) - 
                 beta * mean( midparentHeight ) )

ggplot(GaltonFamilies, aes(x = midparentHeight, y = childHeight)) + 
  geom_point() +
  geom_smooth( method = "lm" ) +
  geom_line( aes( y = midparentHeight ), color = 'red' ) +
  geom_abline(data=GaltonFamilies, aes(slope=beta, intercept=alpha ), color='green' )
```

**Regression to the Mean**: We can see that a child of tall parents is predicted by the least squares line to have a height which is above average but not quite as tall as the parents as
the green line would have you believe. Similarly children of below average height parents are predicted to have a height which is still below average but not quite as short as the parents. This is why Galton used the phrase “regression to mediocrity” and the phenomenon is sometimes called the regression effect.

### Recommended Exercises:
**1.1** The dataset `teengamb` concerns a study of teenage gambling in Britain. Make a numerical and graphical summary of the data, commenting on any features that you find interesting. Limit the output you present to a quantity that a busy reader would find sufficient to get a basic understanding of the data.

The `teengamb` dataset:
```{r}
data( teengamb )
glimpse( teengamb )
```
```{r}
teengamb_clean <- teengamb %>%
  mutate( sex = factor( sex ),
          status = factor( status ) )
summary( teengamb_clean )
```
The numerical summary shows the distribution of subjects in each group for the categorical variables `sex` and `status` as well as the mode/spread of the numerical features: `income` (lbs/wk), `verbal`, and `gamble` (annual).

and now to visualize the data:
```{r}
#visulize the numeric features by group features
si <- ggplot( teengamb_clean, aes( x = sex, y = income, fill = sex ) ) +
  geom_boxplot()
sv <- ggplot( teengamb_clean, aes( x = sex, y = verbal, fill = sex ) ) +
  geom_boxplot()
sg <- ggplot( teengamb_clean, aes( x = sex, y = gamble, fill = sex ) ) +
  geom_boxplot()

grid.arrange( si, sv, sg, ncol = 3 )

sti <- ggplot( teengamb_clean, aes( x = status, y = income, fill = status ) ) +
  geom_boxplot() + theme(legend.position = "none")
stv <- ggplot( teengamb_clean, aes( x = status, y = verbal, fill = status ) ) +
  geom_boxplot() + theme(legend.position = "none")
stg <- ggplot( teengamb_clean, aes( x = status, y = gamble, fill = status ) ) +
  geom_boxplot() + theme(legend.position = "none")

grid.arrange( sti, stv, stg, ncol = 3 )
```
For `sex`, both genders have a very similar distribution of `income` and `verbal` scores. However, one gender has a very different distribution for the annual gambling expenditure.

There appears to be a linear relationship between `verbal` scores and `status` although for other features, the relationship is less clear. However, without knowing the meaning behind the `status` designations, it is difficult to tell.

Look at how some variable covary:
```{r}
ggplot( teengamb_clean, aes( y = gamble, x = income, color = sex ) ) + geom_point( size = 3 ) +
  geom_smooth(method = "lm", alpha = .15, aes(fill = sex))
```
If we plot the amount gambled against weekly income we see two very different trends for the different genders. Males (0) increase their gambling as a function of their weekly income whereas females (1), do not tend to gamble nor show the trend of increasing gambling with increasing income.

Let's take a look at the coefficients for these regression lines:
```{r}
teengamb_clean %>% group_by( sex ) %>%
  do( gambling = broom::tidy( lm( gamble ~ income, data = .))) %>% 
  unnest( gambling )
```
for every GBP increase in weekly income, a male British teen gambles 6.5GBP more annually. However, there is only a negligible increase for females of 0.17.


**1.3** The dataset `prostate` is from a study on 97 men with prostate cancer who were due to receive a radical prostatectomy. Make a numerical and graphical summary of the data as in the first question.

```{r}
data( prostate, package = 'faraway' )
glimpse( prostate )
```
let's learn what the variables are before proceeding:  

* lcavol -> log(cancer volume)
* lweight -> log(prostate weight)
* age -> age (years)
* lbph -> log(benign prostatic hyperplasia amount)
* svi -> seminal vesicle invasion
* lcp -> log(capsular penetration)
* gleason -> Gleason score
* pgg45 -> percentage Gleason scores 4 or 5
* lpsa -> log(prostate specific antigen)

observe summaries of each feature:
```{r}
# svi appears to be binary, so make that adjust:
prostate_clean <- prostate %>%
  mutate( svi = factor( svi ) )
summary( prostate_clean )
```
generate a matrix plot ordered by degree of correlation:
```{r}
dta <- prostate_clean %>%
  select( -c( svi, gleason ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
```
we observe a number of cross-correlations that might need attention for further analysis. For example, `lbph`, or the amount of benign prostatic hyperplasia amount is highly correlated with the weight of the prostate, `lweight`.

the Gleason Score assesses the severity of prostate cancer: A Gleason score of 6 is low grade, 7 is intermediate grade, and a score of 8 to 10 is high grade cancer.

We might be interested in using these features to find out what is predictive of `gleason`. How predictive are certain data features correlate with the Gleason Score?

**1.4** The dataset `sat` comes from a study entitled “Getting What You Pay For: The Debate Over Equity in Public School Expenditures.” Make a numerical and graphical summary of the data as in the first question.

```{r}
data( sat )
glimpse( sat )
```
observe the general properties of each variable
```{r}
summary( sat )
```
generate a matrix plot ordered by degree of correlation:
```{r}
dta <- sat
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
```

Plot the trends for sat scores as a function of expenditure:
```{r}
sat_scores <- sat %>%
  select( expend, verbal, math ) %>%
  pivot_longer( cols = c( verbal, math ), 
                names_to = 'testcat',
                values_to = 'score' )
ggplot( sat_scores, aes( x = expend, y = score, color = testcat ) ) +
  geom_point( size = 2 ) +
  ylab( 'score' )
```
This plot suggests that there is a negative relationship between the expenditures of a state and the average sat scores for both math and verbal.

```{r}
sat_scores %>% group_by( testcat ) %>%
  do( exscor = broom::tidy( lm( score ~ expend, data = .))) %>% 
  unnest( exscor )
```

look at the cummulative scores
```{r}
sat_total_lm <- sat %>%
  do( totalsat = broom::tidy( lm( total ~ expend, data = . ) ) ) %>%
  unnest( totalsat )
sat_total_lm
```
Although the negative relationship is significant, this does not take into account an important confounding variable: the percentage of students that takes the exam:

```{r}
ggplot( sat, aes( y = total, x = takers ) ) +
  geom_point( size = 2 )
```
Perform a multiple linear regression that includes the percentage of people taking the sat
```{r}
satmod <- lm( total ~ expend + takers, sat )
summary( satmod )
```
There is a dramatic improvement in the model fit when percentage of sat test takers is included. However, there is definitely room for improving on this model. For instance, the 'U' shaped envelope of the relationship between total score and percent test takers hints at a nonlinearity. Additionally, there are more data features in the set that could be explored.

**1.5** The dataset 'divusa' contains data on divorces in the United States from 1920 to 1996. Make a numerical and graphical summary of the data as in the first question.

```{r}
data( divusa )
glimpse( divusa )
```
inspect the summary stats:
```{r}
summary( divusa )
```
Use `facet_wrap()` to look at how each of the features varies with time (`year`):
```{r}
divusa_long <- divusa %>%
  pivot_longer( cols = c( divorce, unemployed, femlab, marriage, birth, military ), names_to = 'cat', values_to = 'val' )
ggplot( aes( x = year, y = val ), data = divusa_long ) +
  geom_line() +
  #geom_point() +
  facet_wrap( ~ cat )
```
There are a few trends we see that occur over time:  

* the increase in percentage of women in the workplace
* spikes in military/femlab/divorce/marriage coinciding with WW2
* a peak in unemployment corresponding with the great depression

and now to view pairwise comparisons:
```{r}
dta <- divusa
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
```
There are several relationships that look interesting to explore further:  

* the percentage of women in the workplace has a positive correlation with the divorce rate and a negative relationship with the marriage rate.
* the birth rate has a similar relationship: positively correlated with marriage and negatively correlated to divorce
* Not immediately clear from the previous figure: we see that divorce has a positive correlation with time whereas marriage is negative

## Estimation

### Linear Model
suppose we want to model a response property $Y$ by three feature variables $X_1$ $X_2$ and $X_3$. A general form to represent that is given:
$$Y = f(X_1,X_2,X_3) + \epsilon$$
typically, we won't ever know the true $f$. For linear models, we make the assumption that parameters enter linearly to predict the response:
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon$$
Linear models seem restrictive, but predictors can be transformed and combined such that linear models can be applied. Linear model implies simplicity, however, in practice linear models and the data they describe may in fact be quite complex.  
Where do linear models come from?  

* Physical Theory may suggest a model (Hooke's Law)
* Experience with past data
* No prior data exists (linear model used as a starting point)

*A good model is like a map that guides us to our destination*

### Estimating \beta
We would like to choose $\beta$ such that the systematic component of our model explains as much of the response as possible. where:

| Data =        | Systemic Structure | + | Random Variation |
|---------------|--------------------|---|------------------|
| n dimentions= | p dimensions       | + | (n-p) dimensions |

The difference between the actual response and the predicted response = the residual. 
The conceptual purpose of the linear model is to represent as accurately as possible, some data that is complex with n-dimensions with something that is comparatively simple, a model with just 'p' terms/coefficients.

### Least Squares Estimation
The random variation in the residuals lies in the n-p dimensional space; the aim of fitting a model is the minimize the residuals.  
n-p = *degrees of freedom*  
RSS = Residual Sum of Squares  
estimate of $\sigma^2$: $\hat{\sigma}^2 = \frac{RSS}{n-p}$

### An Example from the Galapagos
```{r}
data( gala )
glimpse( gala )
```
fit a linear model using `lm()`
```{r}
gala_mod <- lm( Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala )
gala_mod_sum <- summary( gala_mod )
gala_mod_sum 
```
the abridged output alternative from the `faraway` library:
```{r}
sumary( gala_mod )
```

Let's directly calculate some quantities of interest:
```{r}
#extract the feature matrix and the response variable:
fmat <- model.matrix( ~ Area + Elevation + Nearest + Scruz + Adjacent, gala )
r <- gala$Species
```
construct $(X^TX)^{-1}$:
```{r}
xtxi <- solve( t( fmat ) %*% fmat )
```
get $\hat{\beta}$ by calculating $(X^TX)^{-1}X^Ty$:
```{r}
xtxi %*% t( fmat ) %*% r
```
A more efficient and accurate way to get $\hat{\beta}$:
```{r}
solve( crossprod( fmat, fmat ), crossprod( fmat, r ) )
```
compare with the `lm()` results:
```{r}
sumary( gala_mod )
```
We can extract a lot of information from the lm structure:
```{r}
residuals( gala_mod ) #extract the residuals
fitted( gala_mod ) #extract the model predictions
df.residual( gala_mod ) #extract the degrees of freedom
deviance( gala_mod ) #gives the RSS
coef( gala_mod ) #returns the coefficient estimates
```

estimate $\sigma$:
```{r}
sqrt( deviance( gala_mod ) / df.residual( gala_mod ) )
gala_mod_sum$sigma
```
compute the standard error for the coefficients
```{r}
xtxi <- gala_mod_sum$cov.unscaled
SE <- sqrt( diag( xtxi ) ) * gala_mod_sum$sigma
SE
```
or extract the SE
```{r}
gala_mod_sum$coef[,2]
```
There are great reasons to use Least Squares:  

* It makes sense geometrically (makes an orthogonal projection onto model space)
* If the errors are independent and identically normally distributed, it estimates the maximum likelihood
* The Gause-Markov rule states that $\hat{\beta}$ is the best linear unbiased estimate

However, there are some situations where Least Squares might not be the best:

* When errors are correlated of have unequal variance
* When the error distribution is long-tailed
* When the predictors are collinear

### Goodness of Fit
a measure of how well the model fits the data.  
*$R^2$* or the **coefficient of determination** or the **percentage of variance explained**:
$$R^2 = 1 - \frac{\mbox{RSS}}{\mbox{Total SS(Corrected for Mean)}} = \mbox{cor}^2 (\hat{y},y)$$
It is a mistake to rely on $R^2$ as the sole measure of goodness of fit.
```{r}
op <- par(mfrow=c(1,4),mar=c (0,0,2,3), oma = c(5, 4.2, 0, 0))
with(anscombe, plot(x1, y1, xlab = "", ylab = "", main = bquote(paste(italic(r),
" = ",.(round(cor(x1, y1),2)))))); abline(3,0.5) 
with(anscombe, plot(x2, y2, xlab = "", ylab = "", main = bquote(paste(italic(r),
" = ",.(round(cor(x2, y2),2)))))); abline(3,0.5) 
with(anscombe, plot(x3, y3, xlab = "", ylab = "", main = bquote(paste(italic(r),
" = ",.(round(cor(x3, y3),2)))))); abline(3,0.5) 
with(anscombe, plot(x4, y4, xlab = "", ylab = "", main = bquote(paste(italic(r),
" = ",.(round(cor(x4, y4),2)))))); abline(3,0.5) 
mtext(expression(italic(y[1])),side=1, outer = TRUE, line = 3)
mtext(expression(italic(y[2])),side=2, outer = TRUE, line = 2.6)
mtext("(a)",side=3, at = -42, line = .5)
mtext("(b)",side=3, at = -26, line = .5)
mtext("(c)",side=3, at = -10.3, line = .5)
mtext("(d)",side=3, at = 5.5, line = .5)
par(op)
# }
```
All 4 of these distributions result in the same lm slope with the same $R^2$ value.

$\hat{\sigma}$ is another measure returned by the `lm()` summary that is in the same units as the response variable.

### Identifiability
Unidentifiability will occur when $X$ is not of full rank. e.g. columns that are linear combinations of one another.

### Orthogonality
```{r}
data( odor )
glimpse( odor )
```
compute the covariance:
```{r}
cov( odor[,-1] )
```
```{r}
odor_mod <- lm( odor ~ temp + gas + pack, odor )
summary( odor_mod, cor = T )
```
The Correlation Coefs are 0.  
These feature variables are entirely independent

### Exercises

#### 2.4
The dataset `prostate` comes from a study on 97 men with prostate cancer who were due to receive a radical prostatectomy. Fit a model with lpsa as the response and lcavol as the predictor. Record the residual standard error and the $R^2$ . Now add lweight, svi, lbph, age, lcp, pgg45 and gleason to the model one at a time. For each model record the residual standard error and the $R^2$. Plot the trends in these two statistics.

```{r}
data( prostate, package = 'faraway' )
glimpse( prostate )
```
fit model and keep track of residual standard error and $R^3$:
```{r}
formula <- ''
formulas <- c()
res_std_err <- c()
r_sqrd <- c()
feature_order <- c( 'lcavol', 'lweight', 'svi', 'lbph', 'age', 'lcp', 'pgg45', 'gleason' )
for (i in 1:length( feature_order )) {
  if (i == 1) {
    formula <- paste0('lpsa', " ~ ", feature_order[i] )
  }
  else {
    formula <- paste0(formula, " + ", feature_order[i] )
  }
  mod_sum <-  summary( lm( formula, data = prostate ) )
  res_std_err[[i]] <- mod_sum$sigma
  r_sqrd[[i]] <- mod_sum$r.squared
  #print( formula )
}

mods_df <- data.frame( 'n_terms' = seq( 1,8 ),
                       'sigma' = unlist(res_std_err, recursive = FALSE),
                       'r_sqrd' = unlist(r_sqrd, recursive = FALSE) )
glimpse( mods_df )
```

visualize the trends of the summary statistics:
```{r}
sigma_plot <- ggplot( mods_df, aes( x = n_terms, y = sigma ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  geom_line() +
  xlab( 'Number of features added to lm()' ) +
  ylab( 'sigma' ) +
  theme_classic() 

r_sqrd_plot <- ggplot( mods_df, aes( x = n_terms, y = r_sqrd ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  geom_line() +
  xlab( 'Number of features added to lm()' ) +
  ylab( 'R^2' ) +
  theme_classic() 

grid.arrange( sigma_plot, r_sqrd_plot, ncol = 2 )
```


#### 2.5
Using the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa on lcavol and lcavol on lpsa. Display both regression lines on the plot. At what point do the two lines intersect?

```{r}
m <- lm(lcavol ~ lpsa, prostate)
m2 <- lm(lpsa ~ lcavol, prostate)

plot1 <- ggplot(prostate, aes(lcavol, lpsa)) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3) +
  geom_line(aes(x = predict(m), color = "lcavol ~ lpsa")) +
  geom_line(aes(y = predict(m2), color = "lpsa ~ lcavol")) +
  theme_classic() 
plot1
```
find the intersection of the two lines:
```{r}
m_aug <- augment( m )
m2_aug <- augment( m2 )
P3 <- c( m2_aug$lcavol[1], m2_aug$.fitted[1] )
P4 <- c( m2_aug$lcavol[length( m2_aug$lcavol )], m2_aug$.fitted[length( m2_aug$lcavol )] )
P1 <- c( m_aug$.fitted[1], m_aug$lpsa[1] )
P2 <- c( m_aug$.fitted[length( m_aug$lcavol )], m_aug$lpsa[length( m_aug$lcavol )] )
intersect <- line.line.intersection(P1, P2, P3, P4, interior.only = TRUE)
int_df <- data.frame( 'X' = intersect[1], 'Y' = intersect[2] )
int_df
```
visually confirm the intersection
```{r}
coords <- paste( '(', round(int_df$X,2), ',', round(int_df$Y,2), ')' )
plot1 + annotate(geom="point", x=int_df$X, y=int_df$Y,
              color="red") +
  annotate(geom="text", x=2, y=2, label=coords,
              color="red")
  ggtitle( 'intersection of lm() model fits' )
```

## Inference

### Hypothesis Tests to Compare Models
**F-statistic**: A statistic used to evaluate the residuals for the model compared to the null hypothesis.  
$$F = \frac{ \frac{(RSS_{\omega}-RSS_{\Omega})}{p-q}}{\frac{ RSS_{\Omega}}{n-p}} \sim F_{p-q,n-p}$$
We would reject the null hypothesis if $F\gt F_{p-q,n-p}^{(\alpha)}$
Where p = num parameters in $\Omege$ and q is the num params in $\omega$. The degrees of freedom of a model at typically the number of observations - the number of parameters, so F can also be written as:
$$F = \frac{ \frac{(RSS_{\omega}-RSS_{\Omega})}{df_{\omega}-df_{\Omega}}}{\frac{ RSS_{\Omega}}{df_{\Omega}}}$$

#### Are any of the Feature Variables Useful in Predicting the Response?
We do not know whether all predictors are required to predict a response or just some. The F-statistic can help determine this.  

Demonstrate this by revisiting the data from the Galapagos:
```{r}
gala_mod_sum
```
model the Null Hypothesis and perform an ANOVA:
```{r}
gala_nullmod <- lm( Species ~ 1, gala )
anova( gala_nullmod, gala_mod )
```
Directly computing the F-stat & it's p-value:
```{r}
rss0 <- deviance( gala_nullmod )
rss <- deviance( gala_mod )
df0 <- df.residual( gala_nullmod )
df <- df.residual( gala_mod )
fstat <- ((rss0-rss)/(df0-df)/(rss/df))
fstat
1 - pf( fstat, df0-df, df )
```

#### Can 1 particular feature be dropped from a model?
Let $\Omega$ be the full model.  
Let $\omega$ be the full model - suspect feature.  
Test whether the feature can be dropped by looking at the F-statistic. 
The null hypothesis is that $H_0 : \beta_i = 0$

```{r}
gala_mod_noArea <- lm( Species ~ Elevation + Nearest + Scruz + Adjacent, gala )
anova( gala_mod_noArea, gala_mod )
```
The p-val of 0.3 indicates that the Null cannot be rejected

#### Testing a pair of predictors
Fit a model without the pair & build the F-test:
```{r}
gala_mod_noAreas <- lm( Species ~ Elevation + Nearest + Scruz, gala )
anova( gala_mod_noAreas, gala_mod )
```
Here, the F-test is statistically significant suggesting that we can reject the null hypothesis. The rejection suggests that there is a difference in the amount of residual error that is accounted for between the two models therefore, removing the pair of feature variables cannot be justified.  

#### Testing a Subspace  

* Can we combine two features? use the `I()`, or interaction function to test linear combinations of feature variables
* Can a feature be set to a particular values? use the `offset()` function

#### What can't we use an F-test to evaluate?  

* Non-linear relationships. ex: $H_0 : \beta_j \beta_k = 1$
* the null model needs to be in the feature subspace of the full model.  

### Permutation Tests: when you don't want to assume normality
**Permutation Test**: what is the chance that the F-test is larger than we observed? We could compute this exactly by computing the F-stat for all permutations of the response variable and see what proportion have greater F-tests.

```{r}
gala_mod_2 <- lm( Species ~ Nearest + Scruz, gala )
gala_mod_2_sum <- summary( gala_mod_2 )
gala_mod_2_sum$fstatistic
1 - pf( gala_mod_2_sum$fstatistic[1],
        gala_mod_2_sum$fstatistic[2],
        gala_mod_2_sum$fstatistic[3])
#anova( gala_mod_noAreas, gala_mod )
```
use the `sample()` function to repeat this 4000x
```{r}
nreps <- 4000
fstats <- numeric( nreps )
for (i in 1:nreps ) {
  lmsum <- summary( lm( sample( Species ) ~ Nearest + Scruz, gala ) )
  fstats[i] <- lmsum$fstat[1]
}

mean( fstats > gala_mod_2_sum$fstatistic[1] )
```

## Prediction
Prediction is one of the main uses for regression models. However, a point estimate is not the end of the story. It is as important to consider the uncertainty of the estimate so that we may understand the range of expected outcomes. Projections are not useful without a realistic estimate of uncertainty because we need to make sensible plans for when events do not turn out as well as predicted. 

### Confidence Intervals for Prediction

* **Prediction of a Mean Response**: Confidence Interval
  + $\hat{y}_0 \pm t_{n-p}^{\frac{\alpha}{2}}\hat{\sigma} \sqrt{x_0^T(X^TX)^{-1}x_0}$
  + There is a 95% chance that the mean value falls within this CI
* **Prediction of a Future Observations**: Prediction Interval
  + $\hat{y}_0 \pm t_{n-p}^{\frac{\alpha}{2}}\hat{\sigma} \sqrt{1 + x_0^T(X^TX)^{-1}x_0}$
  + There is a 95% chance that the future value falls with in this PI
* **Extrapolation**: Predicting an estimate for values that lie outside of the given data range

### Example: Body Fat
load body fat data:
```{r}
data( fat )
#glimpse( fat )
fatmod <- lm( brozek ~ age + weight + height + 
                neck + chest + abdom + hip + thigh + 
                knee + ankle + biceps + forearm + 
                wrist, data = fat )
summary( fatmod )
```
Consider a 'typical' man (mean features).
```{r}
x <- model.matrix( fatmod )
x0 <- apply( x, 2, median )
x0
```

```{r}
fat_select <- fat %>%
  select( c( age, weight, height, neck, chest, abdom, hip, thigh, knee, ankle, biceps, forearm, wrist ) )
meanMan <- colMeans( fat_select )
meanMan
```
if we are predicting the expected value and PI for an individual with mean features:
```{r}
predict( fatmod, newdata = data.frame( t( meanMan ) ), interval = 'prediction' )
```
if we are predicting the mean body fat for all men that have mean characteristics
```{r}
predict( fatmod, newdata = data.frame( t( meanMan ) ), interval = 'confidence' )
```
#### Extrapolation
the further away the values are from the original data, the more uncertainty there is in the estimations.  
Compare the intervals calculated below with those for the mean 'typical' body metrics:
```{r}
extremeFat <- apply( x, 2, function( x ) quantile( x, 0.95 ) )
predict( fatmod, newdata = data.frame( t( extremeFat ) ), interval = 'prediction' )
predict( fatmod, newdata = data.frame( t( extremeFat ) ), interval = 'confidence' )
```


### Autoregression

Consider the trends in this data on monthly airline passengers:

```{r}
data( airpass, package = 'faraway' )
AirlinePlot <- ggplot( airpass, aes( x = year, y = pass ) ) +
  geom_line() +
  ylab( 'Log(Passengers)' ) +
  theme_classic() +
  ggtitle( 'Airline Passengers', subtitle =  'with an obviously inappropriate linear regression fit')

AirlinePlot +   geom_smooth( method = 'lm' )
```

The linear fit above captures the increase in passengers over the years, but does nothing to capture the seasonal variation in the data.

**Autoregressive model**: the response depends on past values of the response. So, build into the model is a feature variable that describes a previous response. For example, it may be the case that we would like to predict the passengers for next month. We would like to know the expected change in ridership to predict next months ridership. Autoregressive 'lagged' variables can be incorporated into the model to describe the number of passengers from previous months:  

$$y_t = \beta_0 + \beta_1y_{t-1} + \beta_{12}y_{t-12} + \beta_{13}y_{t-13} + \epsilon_t $$

```{r}
lagdf <- embed( log( airpass$pass ), 14 )
colnames( lagdf ) <- c( 'y', paste0( 'lag',1:13 ) )
lagdf <- data.frame( lagdf )
#build our linear model:
armod <- lm( y ~ lag1 + lag12 + lag13, lagdf )
summary( armod )
```
visualize the autoregressive model fit:
```{r}
airpass_lag <- airpass[14:144,]
airpass_lag[ 'predict' ] <- exp( predict( armod ) )
AirlinePlot +
  geom_line( aes( x = year, y = predict, color = 'red' ), data = airpass_lag, show.legend = FALSE )
```

predicting future values:
use the last observation in the data as the lag one value:
```{r}
#last value of the data
last_row_df <- lagdf[ nrow( lagdf ), ]
# the current response variable ('y') will become lag1 and everything else shifts over 1:
new_data <- data.frame( lag1 = last_row_df$y, lag12 = last_row_df$lag11, lag13 = last_row_df$lag12 )
#prediction:
next_month_res <- predict( armod, newdata = new_data, interval = 'prediction' )

paste( 'The Autoregression Mod predicts', round( exp( next_month_res[1] ),0), 
       'passengers with a 95% confidence interval from', round( exp( next_month_res[2] ),0),
       'to', round( exp( next_month_res[3] ),0))
```

### What can go wrong with predictions?

* Bad Models: If it doesn't fit, don't apply it.
* Quantitative Extrapolation: We try to predict outcomes for cases with predictor values much different from what we saw in the data
* Qualitative Extrapolation: We try to predict outcomes for observations that come from different populations
* Overconfidence due to overfitting: data is not robust to new data
* Black swans: might not have enough data to properly describe the distribution


## Explanation
The relationships between variables. can include causal conclusions, however these require more rigor.

### Simple Meaning
a simple interpretation of model coefficients: *unit increase in $x_1$ with the other predictors help constant will produce a change of $\hat{\beta_1}$ in the response variable $y$.* This is conceptually difficult: IRL there is no way to hold these variables constant to observe the model's effects and it is not likely that the model captures all contributing variables to the system. Furthermore, our model's explanation contains no notion of explanation.

### Establishing Causality  
causality is determined by experimentally manipulating a variable. Consider a drug trial test with a placebo group and a test group. We can describe the causal effect as:
$$\delta_i = y_i^{\mbox{test}}-y_y^{\mbox{control}}$$

### Designed Experiments
**Randomization is key to success** 

* Randomization is the only reliable way to make sure that the two groups are not unbalances in some way that 
favors either experimental groups.
* Randomization can be used to validate assumptions. (e.g. permutation tests used to test the significance of differences between groups)

On Generalization: The results of randomized experiments apply to the subjects of the experiment unless we can reasonably claim that these subjects are representative of a larger population.  
On Blocking: There may be ways that subjects differ from one another that are identifiable and that can be worked into the experiment. (e.g. Gender).

### Observstional Data
Cannot always collect data in a designed experiment.
Consider this dataset from the Democratic primary of 2008:
```{r}
data( newhamp )
glimpse( newhamp )
```
look at the difference in counts for Digital vs Hand cast ballots for Obama vs Clinton
```{r}
newhamp %>% group_by( votesys ) %>% 
  summarise( 'Obama' = sum( Obama ),
             'Clinton' = sum( Clinton ) ) %>%
  mutate( 'Obama_gt_percent' = (Obama - Clinton)/(Obama + Clinton)*100 )
```
We can see that for digital ballots, Clinton received ~6% more votes than Obama whereas for paper ballots Obama received about 8% more of the vote.  
Is this difference in votes based on voting system significant?  
Here we fit a linear model to proportion of votes for Obama ~ Hand voting.  
First we create a dummy variable for whether the record is for hand votes:
```{r}
newhamp_trt <- newhamp %>%
  mutate( treatment = case_when( votesys == 'H' ~ 1,
                                 votesys == 'D' ~ 0 ) )
newhamp_trt %>% do( tidy( lm( pObama ~ treatment, . ) ) )
```
The p-value for the treatment group suggests that there is a significant difference between the proportion of vote for Obama given the voting system used. However, were there other variables involved? Did the voting system have some causal effect on the outcome?  
In other words, is there a **confounding variable**?

```{r}
glimpse( newhamp )
newhamp_trt %>%
  do( tidy( lm( pObama ~ treatment + Dean + Kerry + povrate + white, . ) ) )
```
If we include other variables we see that our treatment variable is no longer a statistically significant towards predicting the proportion of votes for Obama.

### Matching
for matched pairs where two members of each pair are as alike as possible with respect to confounders.  
we will use the `Matching` library for this:
```{r}
set.seed( 123 )
matches <- GenMatch( newhamp_trt$treatment, newhamp_trt$Dean,
                     ties = FALSE, caliper = 0.05, pop.size = 1000 )
head( matches$matches[ ,1:2 ] )
```
The output above shows some of the pairs of one subject who voted by hand/paper vs one subject who voted digitally matched by thier propensity to vote have voted for Howard Dean in the previous election.

Show all the matches when plotted pObama ~ Dean
```{r}
plot( pObama ~ Dean, newhamp_trt, pch = treatment + 1 )
with( newhamp_trt, segments( Dean[ matches$matches[ ,1 ] ], 
                             pObama[ matches$matches[ ,1 ] ],
                             Dean[ matches$matches[ ,2 ] ],
                             pObama[ matches$matches[ ,2 ] ] ) )
```
From the figure above, we can see that the matches are reasonably well made for values of `Dean`  

Is there any indication of a bias for voting system within the matched pairs?  
Here we compute the difference between pairs
```{r}
pdiff <- newhamp_trt$pObama[ matches$matches[ ,1 ] ] - 
  newhamp_trt$pObama[ matches$matches[ ,2 ] ]
t.test( pdiff )
```
```{r}
plot( pdiff ~ newhamp_trt$Dean[ matches$matches[ ,1 ] ], 
      xlab = 'Dean', ylab = 'Hand - Digital' )
abline( h = 0 )
```
The matched pairs show no clear preference for hand vs digital voting.
The observed difference appears to be because voter inclined to pick Obama are also more likely to be present in hand voting wards.

### Covariate Adjustment, or Controlling for the covariate
Compare univariate and covariate outcomes for digital vs hand voting systems
```{r}
plot( pObama ~ Dean, newhamp_trt, pch = treatment + 1 )
abline( h = c( 0.353, 0.353 + 0.042 ), lty = 1:2 )
abline( 0.221, 0.5229 )
abline( 0.221 - 0.005, 0.5229, lty = 2 )
with( newhamp_trt, segments( Dean[ matches$matches[ ,1 ] ], 
                             pObama[ matches$matches[ ,1 ] ],
                             Dean[ matches$matches[ ,2 ] ],
                             pObama[ matches$matches[ ,2 ] ] ) )
```

### Qualitative Support for Causation

* Strenth - large effects
* Consistency - similar effect found in other studies
* Specificity - 
* Temporality - consistent direction of effect
* Gradient - linear relationship
* Plausibility - does it make sense
* Experiment


## Categorical Predictors

<br><br><br>