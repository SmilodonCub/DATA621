---
title: 'Extending the Linear Model with `R`'
subtitle: 'by Julian Faraway'
author: 'notes by Bonnie Cooper'
output:
  rmdformats::downcute
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<a href="https://github.com/SmilodonCub/DATA621" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

The following are notes from readings in ['Extending the Linear Model with `R`'](https://julianfaraway.github.io/faraway/ELM/) by Julian Faraway for the course DATA621, 'Business Analystics and Data Mining' as part of the [Masters of Science in Data Science program at CUNY SPS](https://sps.cuny.edu/academics/graduate/master-science-data-science-ms).  

`R` Libraries Used:
```{r message=F}
library( faraway )
library( dplyr )
library( ggplot2 )
library( gridExtra )
library( tidyverse )
library( MASS )
library( brglm2 )
library( survival )
library( nnet )
```


## Binomial Data

### The Challenger Disaster Example  
We are interested in how the probability of failure in a given O-ring is related to the launch temperature and predicting the probability when the temperature is $31^\circ$F.

```{r}
data( orings )
glimpse( orings )
```

Visualize the data and fit a simple linear regression model:
```{r}
p1 <- ggplot( data = orings, aes( x = temp, y = damage/6 ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  #geom_jitter(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  geom_smooth( method = 'lm', col = 'red', se = F ) +
  ylab( 'Prob of Damage' ) +
  xlab( 'Temperature' ) +
  ggtitle( 'Naive Approach: simple linear regression' ) +
  theme_classic()

p1
```

However, there are several problems to the simple linear regression approach. Most obviously that the linear regression method can predict probabilities $\gt1 \mbox{ & } \lt 0$ and that is problematic, because those values are not possible.

### Binomial Regression Model

$$P(\mbox{k out of n}) = \frac{n!}{k!(n-k)!}p^k(1-p)^{(n-k)} = 
\left( \begin{array}{c} n_i \\ y_i \end{array} \right)p_i^{y_i}(1-p_i)^{n_i-y_i}$$
$$P(\mbox{event k happening out n possible ways}) = (\mbox{how many possibilities})\cdot P(\mbox{each of n possibilities})$$
to describe the relationship of all the predictors ($x_{i1}\dots x_{iq}$) to p (the success probability), we set up a linear predictor $\eta_i = \beta_0 + \beta_1x_{i1} + \dots + \beta_qx_{iq}$.  

$\eta_i = p_i$ is not appropriate, because it is required that $0\leq p_i \leq 1$, so here are some other choices to use as **linking functions**:  

1. Logit: $\eta = log( p/(1-p))$
2. Propit $\eta = \Phi^{-1}(p)$ where $\Phi$ is the inverse normal cumulative distribution function
3. Complimentary log-log: $\eta = log(-log(1-p))$

**Link Functions**: link the linear predictor to the mean of the response in the wider class of models

Here we will use the logit as a linking function:
```{r}
logitmod <- glm( cbind( damage, 6 - damage ) ~ temp, family = binomial, orings )
sum_logit <- summary( logitmod )
sum_logit$coefficients
```

visualize the logit & probit fit to the data 
```{r}

p2 <- ggplot( data = orings, aes( x = temp, y = damage/6 ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  geom_smooth( method = 'glm', col = 'blue', method.args = list( family = binomial(link = "probit")), se = F ) +
  geom_smooth( method = 'glm', col = 'red', method.args = list( family = 'binomial'), se = F ) +
  ylab( 'Prob of Damage' ) +
  xlab( 'Temperature' ) +
  ggtitle( 'GLM: logistic regression' ) +
  theme_classic()
p2
```

look at the coefficients for the probit glm:
```{r}
probitmod <- glm( cbind( damage, 6 - damage ) ~ temp, family = binomial( link = "probit"), orings )
sum_probit <- summary( probitmod )
sum_probit$coefficients
```

Although the coefficients are quite different, the fits are similar, particularly in the range of the data.  

Now to predict the value at 31 degrees F
```{r}
ilogit( sum_logit$coefficients[1,1] + sum_logit$coefficients[2,1]*31 )
pnorm( sum_probit$coefficients[1,1] + sum_probit$coefficients[2,1]*31 )
```

### Inference
```{r}
sum_logit$coefficients
pchisq( deviance( logitmod ), df.residual( logitmod ), lower = F )
```

construct a profile likelihood-based confidence interval
```{r}
confint( logitmod )
```

### Interpreting Odds
Odds are sometimes a better scale than probability to represent chance. Odds arose as a way to express the payoffs for bets.  

\t\t $o = \frac{p}{1-p}$     $p = \frac{o}{1 + o}$  

Odss also form the basis of a subjective assessment of probability.  
If we have two covariates ($x_1 \mbox{ & } x_2$) then:  
$$log( odds ) = log(\frac{p}{1-p}) = \beta_0 + \beta_1x_1 + \beta_2x_2$$
Incidence of respiratory disease in infants to the age of 1 year
```{r}
data( babyfood )
xtabs( disease/(disease+nondisease ) ~ sex + food, data = babyfood )
```

fit the model:
```{r}
babyfood_mod1 <- glm( cbind( disease, nondisease ) ~ sex + food, family = binomial, data = babyfood )
sum_bfood_mod1 <- summary( babyfood_mod1 )
sum_bfood_mod1
```
is there a sex-food interactions? ....look at the residual deviance. here it is small for the given degrees of freedon, therefore we can conclude that there is no evidence of an interaction effect.  

test the main effects:
```{r}
drop1( babyfood_mod1, test = 'Chi' )
```
the `drop1` function result shows that both predictors are significant

```{r}
exp( sum_bfood_mod1$coefficients[3,1 ] )
```

Breast feeding reduces the odds of respiratory disease to 51% of that for bottle feeding

find confidence intervals:
```{r}
exp( confint( babyfood_mod1 ) )
```

### Prospective and Retrospective Sampling
**Prospective Sampling** - 'cohort study' the predictors are fixed and then the outcome is observed. ex: have a subject group that present with certain features which are monitored accordingly.
**Retrospective sampling** - 'case-control study' the outcome is fixed and then the predictors are observed. a group of test condition observations are observed (e.g. disease ) and a seperate group of control subjects are observed (disease-free). retrospective studies are cheaper and faster and more convenient. However they are less reliable because they typically use historical record which are known to inaccuracies and incompletenesses.

```{r}
babyfood[c(1,3), ]
```

* given the infant is breast fed, the log-odds of having a respiratory disease are log( 47/447 )
* given the infant is bottle fed, the log-odds of having respiratory disease are log( 77/381 )
* **log-odds ratio**: the difference between these two represents the increased risk of respiratory disease incurred by bottle feeding relative to breast feeding
```{r}
log( 77/381 ) - log( 47/447 )
```

### Choice of Link Function

```{r}
data( bliss )
glimpse( bliss )
```

fit all three link functions:
```{r}
blissm1 <- glm( cbind( dead, alive ) ~ conc, family = binomial, data = bliss )
blissm2 <- glm( cbind( dead, alive ) ~ conc, family = binomial( link = probit ), data = bliss )
blissm3 <- glm( cbind( dead, alive ) ~ conc, family = binomial( link = cloglog ), data = bliss )
```

visualize:
```{r}
par( mfrow=c(1,3) )

x <- seq( -2, 8, 0.2 )
p1 <- ilogit( blissm1$coefficients[1] + blissm1$coefficients[2]*x )
p2 <- pnorm( blissm2$coefficients[1] + blissm2$coefficients[2]*x )
p3 <- 1-exp( -exp( blissm3$coefficients[1] + blissm3$coefficients[2]*x ) )
plot( x, p1, type = "l", ylab = "Probability", xlab = "Dose" )
lines( x, p2, lty = 2 )
lines( x, p3, lty = 5 )

matplot( x, cbind( p2/p1, (1-p2)/(1-p1)), type = 'l', xlab = "Dose", ylab = "Ratio" )

matplot( x, cbind( p3/p1, (1-p3)/(1-p1)), type = 'l', xlab = "Dose", ylab = "Ratio" )
```
The fits are very close to each other in the region that spans the data (0:4). However, they deviate in the tails as the ratio plots show.  

The default choice is the logit link. There are three advantages: it leads to simpler mathematics due the intractability of $\Phi$; it is easier to interpret using odds and it allows easier analysis of retrospectively sampled data. 

### Estimation Problems
```{r}
data( hormone )
plot( estrogen ~ androgen, data = hormone, col = orientation )
```
fit a binomial model to see if orientation can be predicted from the two hormone values.  
When the response is binary, we can use it directly in the glm function:

```{r}
hormone_mod <- glm( orientation ~ estrogen + androgen, data = hormone, family = binomial )
summary( hormone_mod )
```
The residual deviance is very small, indicating a good fit. However, the algorithm did not converge.

A look at the plot of the data reveals that it is linearly seperable so that a perfect fit is possible: 
```{r}
plot( estrogen ~ androgen, data = hormone, col = orientation )
abline( -hormone_mod$coefficients[1]/hormone_mod$coefficients[2], 
        -hormone_mod$coefficients[3]/hormone_mod$coefficients[2],
        col = 'magenta')
```
Use David Firths method which always gives finite estimates

### Goodness of Fit

Deviance is a good measure, but there are others:
Pearson's $\chi^2$ which is analogous to the residual sum of squares in normal linear models  

```{r}
#pearson's chi squared
sum( residuals( blissm1, type = 'pearson' )^2 )
#deviance
deviance( blissm1 )
```
We can see that there is little difference

Niglekerke's 1991 statistic for how well the model explains the data
```{r}
(1 - exp((blissm1$dev - blissm1$null)/150))/ (1-exp(-blissm1$null/150))
```
That is a very good fit

### Prediction and Effective Doses
We wish to predict the outcome for given values of the covariates. use the inverse of the link function to get the prediction on the probability scale:
```{r}
blissm1_sum <- summary( blissm1 )
x0 <- c( 1, 2.5 )
eta0 <- sum( x0*coef( blissm1 ) )
ilogit( eta0 )
```

There is a 64% chance of a death at this dose. now to find the confidence interval:
```{r}
bliss_pred <- predict( blissm1, newdata = data.frame( conc = 2.5 ), se = T )
c1 <- bliss_pred$fit
c2 <- bliss_pred$se.fit
ilogit( c( c1 - 1.96*c2, c1 + 1.96*c2 ) )
```

$$\mbox{Lethal Dose} = \hat{ED50} = \frac{-\hat{\beta}_0}{\hat{\beta}_1}$$
$$\mbox{Effective Dose} = x_p = \frac{\mbox{logit}(p)-\beta_0}{\hat{\beta}_1}$$
```{r}
dose.p( blissm1, p = c( 0.5, 0.9 ) )
```

### Overdispersion
```{r}
data( troutegg )
ftable( xtabs( cbind( survive, total ) ~ location + period, data = troutegg ) )
```

fit a glm for the two main effects 
```{r}
eggmod <- glm( cbind( survive, total-survive ) ~ location + period, family = binomial, data = troutegg )
summary( eggmod )
```

check for outliers:
```{r}
halfnorm( residuals( eggmod ) )
```

plot the empiracal logits:
```{r}
elogits <- log( ( troutegg$survive + 0.5 )/( troutegg$total - troutegg$survive + 0.5 ) )
with( troutegg, interaction.plot( period, location, elogits ) )
```
Having eliminated the usual suspects, we now investigate overdispersion.  
estimate a dipersion parameter:
```{r}
sigma2 <- sum( residuals( eggmod, type = 'pearson' )^2 )/12
summary( eggmod, dispersion = sigma2 )
```

Note the change in p-vals for several predictors


### Matched Case-Control Studies  
Confounding variables are explicitly adjusted for in the experiment design.  
In a **matched case-control study** we match each case with one or more controls that have the same or similar values of some set of potential confounding variables. 
```{r}
data( amlxray )
glimpse( amlxray )
```
```{r}
ii <- which( amlxray$downs=="yes" )
ramlxray <- amlxray[ -c( ii, ii+1 ),]
xraymod <- clogit( disease ~ Sex + Mray + CnRay + strata( ID ), data = ramlxray )
summary( xraymod )
```

drop some insignificant factors
```{r}
xraymod2 <- clogit( disease ~ Fray + unclass(CnRay) + strata( ID ), ramlxray )
summary( xraymod2 )
```


## Count Regression
When the response is a count (a positive integer), we can use a count regression model to explain this response in terms of the given predictors


### Poisson Regression
$$P(Y=y) = \frac{e^u\mu^y}{y!}$$
Incidences when Poisson naturally occurs:  

1. Small success probabilities and large totals, the Poisson is a good approximation
2. Suppose the probability of occurrence of an event in a given time interval is proportional to the length of that time interval and independent of the occurrence of other events. Then the number of events in any specified time interval will be Poisson distributed.
3. Poisson distributions occur when the time between events is independent and identically exponentially distributed.

Galapagos Islands example:
```{r}
data( gala, package = "faraway")
gala <- gala[,-2]
glimpse( gala )
```

```{r}
gala_mod <- lm( Species ~ ., data = gala )
plot( predict( gala_mod ), residuals( gala_mod ), xlab = "Fitted", ylab = "Residuals" )
```
there is evidence of nonconstant variance....here square-root transformation is best:
```{r}
gala_mod_sqrt <- lm( sqrt( Species ) ~., gala )
plot( predict( gala_mod_sqrt ), residuals( gala_mod_sqrt ), xlab = "Fitted", ylab = "Residuals" )
summary( gala_mod_sqrt )
```

The nonconstant variance has been cleared up and the model gives a decent fit.

Now to try a Poisson model:  
```{r}
gala_mod_pois <- glm( Species ~., family = poisson, gala )
summary( gala_mod_pois )
```

**G-statistic** - deviance for the Poisson regression or use **Pearson's $\chi^2$ statistic**

```{r}
halfnorm( residuals( gala_mod_pois ) )
```

Relationship between the mean and variance:
```{r}
plot( log( fitted( gala_mod_pois ) ), log( ( gala$Species-fitted( gala_mod_pois ) )^2 ),
      xlab = expression( hat( mu ) ), ylab = expression( (y - hat( mu ) )^2 ) )
abline( 0,1 )
```
The variance is proportional to, but larger than the mean.

Estimate a dispersion parameter:
```{r}
disp <- sum( residuals( gala_mod_pois, type = "pearson")^2)/gala_mod_pois$df.residual 
disp
```

adjust the standard error
```{r}
summary( gala_mod_pois, dispersion = disp )
```
drop insignificant variables:
```{r}
drop1( gala_mod_pois, test = "F" )
```
Now use the F test to compare between this and the full model. There doesn't seem to be a big change in fit.

### Rate Models
The number of events observed may depend on the size variable that determines the number of opportunities for the event to occur. (e.g. the modeling of rare diseases: we may know the number of cases, but not have precise population data)  

Consider an example experiment on the effect of gamma radiation on chromosomal abnormalities:
```{r}
data( dicentric, package = "faraway" )
round( xtabs( ca/cells ~ doseamt + doserate, dicentric ), 2 )
with( dicentric, interaction.plot( doseamt, doserate, ca/cells ) )
```

In modeling the rate directly, we see that the effect of the dose rate may be multiplicative, so let's try to incorporate that into a model:

```{r}
dicentric_mod <- lm( ca/cells ~ log( doserate)*factor( doseamt), dicentric )
summary( dicentric_mod )$adj
```
The multiplicative model's $R^2$ tells us that the model explains a lot of the variance in the data. but let's visualize the residuals to gain a better understanding ...
```{r}
plot( residuals( dicentric_mod ) ~ fitted( dicentric_mod ),
      xlab = "Fitted", ylab = "Residuals")
abline( h = 0 )
```

Clearly there is a pattern of the residuals increasing.
Here we model the count response by taking the log of the number of cells:
```{r}
dicentric$dosef <- factor( dicentric$doseamt )
dicentric_mod_countr <- glm( ca ~ log( cells ) + log( doserate )*dosef, 
                             family=poisson, dicentric )
summary( dicentric_mod_countr )
```

Now to use a implement a **Rate Model**, we use an offset to correct the coefficient to 1. this makes things much more directly interpretable for the model:
```{r}
dicentric_ratemod <- glm( ca ~ offset( log( cells ) ) + log( doserate )*dosef,
                          family = poisson, dicentric )
summary( dicentric_ratemod )
```

The coefficients are very similar and we see that the residual deviance of the model fits well

### Negative Binomial
Consider this dataset looking at faulty soldering on circuit boards:
```{r}
data( solder, package = "faraway" )
solder_mod <- glm( skips ~ ., family = poisson, data = solder )
deviance( solder_mod )
df.residual( solder_mod )
```
This is not a good fit.  
Try adding including interaction terms:
```{r}
solder_mod_int <- glm( skips ~ ( Opening + Solder + Mask + PadType + Panel )^2,
                       family = poisson, data = solder )
deviance( solder_mod_int )
pchisq( deviance( solder_mod_int ), df.residual( solder_mod_int ), lower = F )
```
Better, but still not convincing...
Negative Binomial....
```{r}
solder_mod_negbin <- glm( skips ~ ., negative.binomial( 1 ), solder )
solder_mod_negbin
```

now allow for different values of *k*:
```{r}
solder_mod_negbin2 <- glm.nb( skips ~ ., solder )
summary( solder_mod_negbin2 )
```

```{r}
data( ships, package = 'MASS' )
glimpse( ships )
summary( ships )
```
```{r}
# exclude ships with 0 months of service
ships2 <- subset(ships, service > 0)
# convert the period and year variables to factors
ships2$year <- as.factor(ships2$year)
ships2$period <- as.factor(ships2$period)
summary( ships2 )

ggplot( data = ships2, aes( x = incidents ) ) +
  geom_bar() +
  theme_classic() +
  ggtitle( "Ship Damage Data", subtitle = 'damage caused by waves to cargo vessels at sea')
```

```{r}
# consider a log-linear model including all the variables
# "offset" means a term with a fixed coefficient of 1
glm1 <- glm(incidents ~ type + year + period, 
    family = poisson(link = "log"), data = ships2, offset = log(service))
summary(glm1)
par(mfrow=c(2,2))
plot(glm1)
```


## Multinomial Data
an extension of the binomial to the situation where the response can take more than two values.  
**Nomial Multinomial Data** - there is no natural order to the groups  
**Ordinal Multinomial Data** - there is a natural order to the groups

### Mulitnomial Logit Model  
link the probabilities with the predictors while ensureing that the probabilities are restricted between 0 and 1.  

```{r}
data( nes96 )
nes96 <- nes96 %>%
  mutate( sPID = case_when( PID == 'strRep' ~ 'Republican',
                            PID == 'indRep' ~ 'Independent',
                            PID == 'weakRep' ~ 'Republican',
                            PID == 'weakDem' ~ 'Democrat',
                            PID == 'strDem' ~ 'Democrat',
                            PID == 'indDem' ~ 'Independent',
                            PID == 'indind' ~ 'Independent' ))
glimpse( nes96 )
nes96 %>% dplyr::select( sPID ) %>% table()
```
```{r}
#convert income to a numeric variable
inca <- c( 1.5, 4,6,8,9.5,10.5,11.5,12.5,13.5,14.5,16,18.5,21,23.5,27.5,32.5,37.5,42.5,47.5,55,67.5,82.5,97.5,115)
nincome <- inca[unclass(nes96$income)]
summary(nincome)
table( nes96$educ)
```

```{r}
matplot( prop.table( table( nes96$educ, nes96$sPID),1),
         type = 'l', xlab = "Education", ylab = "Proportion",
         lty = c(1,2,5))
cutinc <- cut( nincome,7 )
il <- c( 8, 26, 42, 58, 74, 90, 107 )
matplot( il, prop.table(table(cutinc,nes96$sPID),1),
         type = 'l', xlab = "Income", ylab = "Proportion",
         lty = c(1,2,5))
cutage <- cut( nes96$age, 7 )
al <- c( 24,34,44,54,65,75,85 )
matplot( al, prop.table(table(cutage,nes96$sPID),1), ,
         type = 'l', xlab = "Age", ylab = "Proportion",
         lty = c(1,2,5))
```

```{r}
mmod <- multinom( sPID ~ age + educ + nincome, nes96 )
```
```{r}
mmodi <- step(mmod)
```

fit a model without `educ` and compare the deviences:
```{r}
predict( mmodi, data.frame( nincome = il ), type = "probs")
```

```{r}
summary(mmodi)
```

you can look at the logodds, but those are difficult to interpret 

let's instead look at the probabilities:
```{r}
cm <- diag(3)[unclass(factor(nes96$sPID)),]
y <- as.numeric( t(cm) )
resp.factor <- gl( 944,3 )
cat.factor <- gl( 3,1,3*944, labels = c("D","I","R"))
rnincome <- rep( nincome, each = 3 )
head( data.frame( y, resp.factor, cat.factor, rnincome))
```

```{r}
glmod <- glm( y ~ resp.factor + cat.factor + cat.factor:rnincome, family=poisson)
```

compare deviances for 2 approaches:
```{r}
deviance( glmod )
deviance( mmodi )
```

compare coefficients:
```{r}
coef( glmod )[c(1,945:949)]
coef( mmodi )
```

Therefore, the multinomial logit model can be viewed as a GLM-type model, which allows us to apply all the common methodology developed for GLMs


### Hierarchical or Nested Responses

```{r}
data( cns )
glimpse( cns)
```
The majority of cases had no defect (NoCNS), so here it makes sense to treat this as a hierarchical model with first a binomial likelihood, followed by a multinomial that spits up the types od defects:

```{r}
cns$CNS <- cns$An + cns$Sp + cns$Other
plot( log( CNS/NoCNS) ~ Water, cns, pch=as.character(Work))
```
higher for manual workers

```{r}
binmodw <- glm( cbind( CNS, NoCNS ) ~ Water + Work, data = cns, family = binomial )
binmoda <- glm( cbind( CNS, NoCNS ) ~ Area + Work, data = cns, family = binomial )
anova( binmodw, binmoda, test = "Chi")
```
```{r}
halfnorm( residuals(binmodw))
```

```{r}
summary( binmodw )
```
```{r}
round((1-exp( -0.339058 ))*100,2)
```
births to nonmanual workers have a 29% lower chance of CNS malformation.
both the Water quality and Workstyle of the parents are related to the odds of cns malformations.

Now to look at the multinomial relationship (types of cns malformations)

```{r}
cmmod <- multinom( cbind( An, Sp, Other) ~ Water + Work, data = cns )
nmod <- step( cmmod )
```

step AIC leaves us with the null model.  
the fitted proportions:
```{r}
nmod
```
```{r}
cc <- c( 0,0.28963, -0.98083 )
names( cc ) <- c( 'An', 'Sp', 'Other' )
exp( cc )/sum( exp( cc ) )
```
therefore, we find that water hardness and parents' professions are related to the probability of a malformed birth, but has no effect on the type of malformation. This finding would have been missed had we used a simple multinomial logit model to all 4 categories.

### Ordinal Multinomial Responses  
With ordinal responses it is often easiest to work with cumulative probabilities

#### Proportional Odds Model (logit link)
```{r}
pomod <- polr( factor(sPID) ~ age + educ + nincome, data = nes96 )
```

```{r}
c( deviance( pomod ), pomod$edf )
c( deviance( mmod ), mmod$edf )
```
```{r}
pomodi <- step( pomod )
```

```{r}
deviance( pomodi ) - deviance( pomod )
pchisq( 11.151, pomod$edf-pomodi$edf, lower = F)
```
```{r}
summary( pomodi )
```
```{r}
#for an income of $0 the probability of being a democrat:
ilogit( 0.209 )
```
```{r}
#while that of being an Independent:
ilogit( 1.292 ) - ilogit( 0.209 )
```
so the probability of being a republican == 1 - ( 0.55 + 0.23 ) 

```{r}
predict( pomodi, data.frame( nincome = il, row.names = il ), type = "probs")
```

Notice how the probability of being a democrat decreases with income while the probability of being a republican increases with income.

#### Oredered Probit Model
```{r}
opmod <- polr( factor(sPID) ~ nincome, method = "probit", data = nes96 )
summary( opmod )
```
```{r}
dems <- pnorm( 0.128-il*0.008182 )
demind <- pnorm( 0.798-il*0.008182 )
cbind( dems, demind-dems, 1-demind )
```
The predicted values are very similar


<br><br><br>