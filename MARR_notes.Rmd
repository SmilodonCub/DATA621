---
title: 'A Modern Approach to Regression with `R`'
subtitle: 'by Simon Sheather'
author: 'notes by Bonnie Cooper'
output:
  rmdformats::downcute
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following are notes from readings in ['A Modern Approach to Regression with `R`'](https://link.springer.com/book/10.1007/978-0-387-09608-7) by Simon Sheather for the course DATA621, 'Business Analystics and Data Mining' as part of the [Masters of Science in Data Science program at CUNY SPS](https://sps.cuny.edu/academics/graduate/master-science-data-science-ms).

`R` libraries used:
```{r message=FALSE}
library( broom )
library( dplyr )
library( ggplot2 )
library( gridExtra )
library( tidyverse )
library( gclus )
```

## Introduction
building valid regression models for real-world data.  

### Building Valid Models
It makes sense to base inferences or conclusions only on valid models  
Any conclusion is only as sound as the model on which it is based.  

#### Motivating Examples

**1) NFL Field Goals**
```{r}
nflfg_csv <- read.csv( 'FieldGoals2003to2006.csv' )
nflfg_df <- data.frame( nflfg_csv )
glimpse( nflfg_df )
```
```{r}
head( nflfg_df )
unique( nflfg_df$Yeart )
unique( nflfg_df$Name )
```
the incorrect approach just looks at the correlation between feild goal percentages one year and the previous:
```{r}
nflfg_yeardif <- nflfg_df %>%
  dplyr::select( c( Name, Yeart, FGt ) ) %>%
  pivot_wider( names_from = Yeart, values_from = FGt )
nflfg_yeardif

firstY <- nflfg_yeardif %>%
  dplyr::select( c( `2003`, `2004`, `2005` ) ) %>%
  pivot_longer( cols = everything(), names_to = 'Year1', values_to = 'val1' )

NextY <- nflfg_yeardif %>%
  dplyr::select( c( `2004`, `2005`, `2006` ) ) %>%
  pivot_longer( cols = everything(), names_to = 'Year2', values_to = 'val2' )


BothY <- cbind( firstY, NextY )

ggplot( BothY, aes( x = val1, y = val2 ) ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 4 ) +
  labs( title = 'Current by Previous Year' ) +
  xlab( 'Field Goal Percentage in Year t-1' ) +
  ylab( 'Field Goal Percentage in Year t' )

```
Overall, the correlation is very weak. However, this does not take into account the abilities of each of the athletes. another approach would be to fit a linear regression to each athlete's performance across the years.

```{r}
name_labs <- rep( nflfg_yeardif$Name, each = 3 )
BothY$Names <- name_labs

ggplot( BothY, aes( x = val1, y = val2, fill = Names ) ) +
  geom_point( shape = 21, alpha = 0.5, size = 4 ) +
  labs( title = 'Current by Previous Year' ) +
  xlab( 'Field Goal Percentage in Year t-1' ) +
  ylab( 'Field Goal Percentage in Year t' )
```
Now to look at the linear regression coefficients by athlete:

```{r}
by_athlete <- BothY %>%
  group_by( Names )

lm_byAthlete <- do( by_athlete, tidy( lm( val2 ~ val1, data = . ) ) )

lm_byAthlete <- lm_byAthlete %>%
  dplyr::select( c( Names, term, estimate ) ) %>%
  pivot_wider( names_from = term, values_from = estimate )


lm_byAthlete
```
Allowing for a different intercept for each athlete (different abilities), it can be shown that if a kicker had a high field goal percentage the previous year, then they are predicted to have a lower goal percentage the current year.


**2) Newspaper Circulation**
demonstrate the use of dummy variables along with transformations to overcome skewness

```{r}
circ_url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA621/master/circulation.csv'
circ_df <- read.table( circ_url, sep = '\t', header = TRUE )
circ_df <- rename( circ_df, Tabloid_hasComp = Tabloid.with.a.Serious.Competitor )
glimpse( circ_df )
```
The feature `Tabloid.with.a.Serious.Competitor` is a **dummy variable** it only takes a true/false value to indicate an outcome.

Take a look at Sunday ~ Weekday circulation grouped by the dummy variable: 
```{r}
ggplot( circ_df, aes( x = Weekday, y = Sunday, color = factor( Tabloid_hasComp ) ) ) +
  geom_point( size = 3)
```
We can make the variable much more constant (linear) by plotting the log values:

```{r}
ggplot( circ_df, aes( x = log(Weekday), y = log(Sunday), color = factor( Tabloid_hasComp ) ) ) +
  geom_point( size = 3)
```


**3) Menu Pricing in an NYC Restaurant**
highlights the use of multiple regression. this is a classic dataset: pricing East/West of 5th Ave.  
Produce a regression model to predict the price of dinner

```{r}
nyc_csv <- read.csv( 'nyc.csv' )
nyc_df <- data.frame( nyc_csv )
glimpse( nyc_df )
```
```{r}
dta <- nyc_df %>%
  dplyr::select( c( Price, Food, Decor, Service ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
  
```

check out the distributions of the East/West variable:
```{r}
ggplot( nyc_df, aes( x = factor( East ) , y = Price ) ) +
  geom_boxplot() +
  xlab( 'Direction off 5th Ave)' ) +
  ggtitle( 'Price ~ East vs West' ) +
  scale_x_discrete( labels = c( 'West', 'East' ) )
```

```{r}
mod <- lm( Price ~ Service + Decor + Food + East, nyc_df )
summary( mod )
```

**4) Wine Critics' Ratings**
```{r}
wine_csv <- read.csv( 'Bordeaux.csv' )
wine_df <- data.frame( wine_csv )
glimpse( wine_df )
```

```{r}
dta <- wine_df %>%
  dplyr::select( c( Price, ParkerPoints, CoatesPoints ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
  
```

```{r}
wine_df <- wine_df %>%
  mutate_at( vars( P95andAbove, FirstGrowth, 
                   CultWine, Pomerol, VintageSuperstar ),
             funs( factor ) )

firstG <- ggplot( wine_df, aes( y = Price, x = FirstGrowth ) ) +
  geom_boxplot()
p95 <- ggplot( wine_df, aes( y = Price, x = P95andAbove ) ) +
  geom_boxplot()
cult <- ggplot( wine_df, aes( y = Price, x = CultWine ) ) +
  geom_boxplot()
pom <- ggplot( wine_df, aes( y = Price, x = Pomerol ) ) +
  geom_boxplot()
VS <- ggplot( wine_df, aes( y = Price, x = VintageSuperstar ) ) +
  geom_boxplot()

grid.arrange( firstG, p95, cult, pom, VS, ncol = 3 )
```

```{r}
dta <- wine_df %>%
  dplyr::select( c( Price, ParkerPoints, CoatesPoints ) ) %>%
  mutate_at( vars( Price, ParkerPoints, CoatesPoints ),
             funs( log ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of log(features)" )
  
```

Over the chapters in this book, will dive in to these data sets much deeper to model various aspects and build predictive capabilities



## Simple Linear Regression
modeling the relationship between two variables as a straight line, that is, when $Y$ is modeled as a linear function of $X$.

### Introduction to Least Squares Estimates

```{r}
production_url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA621/master/production.txt'
production_df <- read.table( production_url, sep = '\t', header = TRUE )
glimpse( production_df )
```

```{r}
ggplot( production_df, aes( x = RunSize, y = RunTime ) ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 4 ) +
  xlab( 'Run Size' ) +
  ylab( 'Run Time' )
```
 We wish to develop an equation to model the relationship between Y, the run time, and X, the run size.  
 
If the regression of Y on X is linear:
$$Y_i = \mbox{E}(Y|X=x) + e_i = \beta_0 + \beta_1 x + e_i$$
where $e_i$ is the random error in $Y_i$  
all unexplained variation is called random error. Random error does not depend on X, nor does it contain any information about Y (otherwise it would be systematic error).  
**residuals**: the difference between the actual value of y and the predictor value of y.  
we wish to describe a line of best fit which minimizes the residuals.  

fit the production data with a linear model and visualize the residuals:  
```{r}
mod <- lm( RunTime ~ RunSize, production_df )
production_df$pred <- predict( mod )
production_df$res <- residuals( mod )
glimpse( production_df )
```
```{r}
ggplot( production_df, aes( x = RunSize, y = RunTime ) ) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = RunSize, yend = pred), alpha = .2) + 
  geom_point() +
  geom_point(aes(y = pred), shape = 1) +
  theme_classic()  
```
**Least Squares** line of best fit
coefficients for the line of best fit are chosen so as to minimize the sum of the squared residuals:
$$\mbox{RSS} = \sum^n_{i=1}\hat{e}^2_i = \sum^n_{i=1}(y_y-\hat{y}_i)^2 = \sum^n_{i=1}(y_i - b_0 - b_1x_i)^2$$
```{r}
summary( mod )
```
The equation for the best fit line is given by:
$$y = \beta_0 + \beta_1 x = 149.7 + 0.26x$$
* The **intercept**, 149.7, is where the line of best fit crosses the y-axis.
* The **slope**, 0.26, estimates the change in relationship of `RunTime` as a function of `RunSize`. The slope is informative and describes a 0.26 increase in `RunTime` for every unit increase of `RunSize`.

Estimating the variance of the error $\sigma^2 = Var(e)$  
The residuals can be used to estimate $\sigma^2$
$$S^2 = \frac{\mbox{RSS}}{n-2} = \frac{1}{n-2}\sum^n_{i=1}\hat{e}_i^2$$
 
 
### Exercise 2.1
```{r}
playbill_df <- read.csv( 'playbill.csv' )
glimpse( playbill_df )
```
fit a linear model to describe the relationship CurrentWeek ~ LastWeek:

```{r}
playbill_mod <- lm( CurrentWeek ~ LastWeek, playbill_df )
playbill_mod_sum <- summary( playbill_mod )
playbill_mod_sum
```
visualize the results:
```{r}
ggplot( playbill_df, aes( x = LastWeek, y = CurrentWeek ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  xlab( 'Gross Box Office Results Last Week' ) +
  ylab( 'Gross Box Office Results Current Week' ) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  theme_classic() 
```

#### a) find the 95% confidence interval for the slope of the regression model. Is 1 a plausible value for the slope? Give a reason to support your answer

The 95% confidence interval is given by the estimated slope coefficient $\pm$ 2*SE:
```{r}
#can be calculated directly from the coefficients
slope_coeff <- playbill_mod_sum$coefficients[ 2,1 ]
slope_SE <- playbill_mod_sum$coefficients[ 2,2 ]
slope_CI <- c( slope_coeff - 2*slope_SE, slope_coeff + 2*slope_SE )
slope_CI

#alternatively can use the function confint
confint( playbill_mod, 'LastWeek', level = 0.95 )
```
 We can say with 95% confidence that slope of the regression line is within the range of `r slope_CI[ 1 ]` and `r slope_CI[ 2 ]`
 Given this range, we can conclude that 1 is a plausible value for the slope of the line as it is within the CI.
 
 #### b) Test the null hypothesis that $H_0: \beta_0 = 10000$ against a two-sided alternative. Interpret your results.
 taking a look at the confidence interval for intercept:
```{r}
confint( playbill_mod, '(Intercept)', level = 0.95 )
```
10,000 is well withing the 95% confidence interval. Therefore we accept the null hypothesis that $\beta_0 = 10000$

#### c) Use the fitted regression model to estimate the gross box office results for the current week (in $) for a production with $400,000 in gross box office the previous week. Find a 95% prediction interval for the gross box office results for the current week (in $) for a production with $400,000 in gross box office the previous week. Is $450,000 a feasible value for the gross box office results in the current week, for a production with $400,000 in gross box office the previous week? Give a reason to support your answer.

```{r}
lw400k <- data.frame( 'LastWeek' = 400000 )
pred <- predict( playbill_mod, newdata = lw400k, interval = 'predict' )
res <- paste( '1) Estimated GBO w/ LastWeek(400000):', round( pred[ 1 ], 0 ), 
              '\n2) 95% Prediction Interval: Lwr =', round( pred[ 2 ], 0 ), 
              'Upr =', round( pred[ 3 ], 0 ) )
cat( res, sep = '\n' )
```
3) from the prediction interval calculated above, we see that $450,000 is not feasible, because it lies well above the upper limit of the prediction interval (Upr = `r round( pred[ 3 ], 0 )`)
 
#### d) Some promoters of Broadway plays use the prediction rule that next week's gross box office results will be equal to this week's gross box office results. Comment on the appropriateness of this rule. 

The assertion is approximately correct. The slope of the regression line that models a current week's BO gross result is approximately 1 (slightly less at slope = `r playbill_mod_sum$coefficients[ 2,1 ]`) and 1.0 is well with in the confidence interval.

### Exercise 2.2
A story by James R. Hagerty entitled 'With Buyers Sidelined, Home Prices Slide' published in the Wall Street Journal contained data on so-called fundamental housing indicators in major real estate markets across the US. The author argues that, 'prices are generally falling and loan payments are piling up.' Thus, we shall consider data presented in the article.

loading the data:
```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/indicators.txt'
indicators_df <- read.csv( path, sep = '\t', header = TRUE )
glimpse( indicators_df )
```

Fit the following model: $Y = \beta_0 + \beta_1x + \epsilon$  
Where:  

* Y = Percent change in average price 
* x = Percent of mortgage loans 30 days or more overdue
```{r}
indicators_mod <- lm( PriceChange ~ LoanPaymentsOverdue, data = indicators_df )
indicators_mod_sum <- summary( indicators_mod )
indicators_mod_sum
```
visualize the data as well:
```{r}
ggplot( indicators_df, aes( x = LoanPaymentsOverdue, y = PriceChange ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  xlab( 'Percentage of Mortgage Loans >= 30days Overdue' ) +
  ylab( 'Percent Change in Average Price' ) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  theme_classic() 
```

#### a) Find a 95% confidence interval for the slope of the regression model
```{r}
confint( indicators_mod, 'LoanPaymentsOverdue', level = 0.95 )
```
judging by the range of the 95% confidence intervals, a negative linear association between the Percent change in average price by the percentage of overdue mortgage loans is not statistically significant because the 95% CI range extents into positive slope values.

#### b) use the fitted regression model to estimate $E(Y|X=4)$.
Find a 95% CI for $E(Y|X=4)$. Is 0% a feasible value for $E(Y|X=4)$? Give a reason to support your answer.

use the model to predict the expected value at X = 4:
```{r}
exp4 <- data.frame( 'LoanPaymentsOverdue' = 4 )
pred <- predict( indicators_mod, newdata = exp4, interval = 'predict' )
res <- paste( '1) E(Y|X=4):', round( pred[ 1 ], 0 ), 
              '\n2) 95% Prediction Interval: Lwr =', round( pred[ 2 ], 0 ), 
              'Upr =', round( pred[ 3 ], 0 ) )
cat( res, sep = '\n' )
```
A value of 0% is well within the CI for the expected value of Y given X=4. Therefore, with only this data, there is no statistical significance to support a negative relationship between 
changes in housing prices and percentages of overdue loans.
 
 
 
<br><br><br>
