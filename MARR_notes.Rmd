---
title: 'A Modern Approach to Regression with `R`'
subtitle: 'by Simon Sheather'
author: 'notes by Bonnie Cooper'
output:
  rmdformats::downcute
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following are notes from readings in ['A Modern Approach to Regression with `R`'](https://link.springer.com/book/10.1007/978-0-387-09608-7) by Simon Sheather for the course DATA621, 'Business Analystics and Data Mining' as part of the [Masters of Science in Data Science program at CUNY SPS](https://sps.cuny.edu/academics/graduate/master-science-data-science-ms).

`R` libraries used:
```{r message=FALSE}
library( broom )
library( dplyr )
library( ggplot2 )
library( gridExtra )
library( tidyverse )
library( gclus )
library( car )
library( VGAM )
```

## Introduction
building valid regression models for real-world data.  

### Building Valid Models
It makes sense to base inferences or conclusions only on valid models  
Any conclusion is only as sound as the model on which it is based.  

#### Motivating Examples

**1) NFL Field Goals**
```{r}
nflfg_csv <- read.csv( 'FieldGoals2003to2006.csv' )
nflfg_df <- data.frame( nflfg_csv )
glimpse( nflfg_df )
```
```{r}
head( nflfg_df )
unique( nflfg_df$Yeart )
unique( nflfg_df$Name )
```
the incorrect approach just looks at the correlation between feild goal percentages one year and the previous:
```{r}
nflfg_yeardif <- nflfg_df %>%
  dplyr::select( c( Name, Yeart, FGt ) ) %>%
  pivot_wider( names_from = Yeart, values_from = FGt )
nflfg_yeardif

firstY <- nflfg_yeardif %>%
  dplyr::select( c( `2003`, `2004`, `2005` ) ) %>%
  pivot_longer( cols = everything(), names_to = 'Year1', values_to = 'val1' )

NextY <- nflfg_yeardif %>%
  dplyr::select( c( `2004`, `2005`, `2006` ) ) %>%
  pivot_longer( cols = everything(), names_to = 'Year2', values_to = 'val2' )


BothY <- cbind( firstY, NextY )

ggplot( BothY, aes( x = val1, y = val2 ) ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 4 ) +
  labs( title = 'Current by Previous Year' ) +
  xlab( 'Field Goal Percentage in Year t-1' ) +
  ylab( 'Field Goal Percentage in Year t' )

```
Overall, the correlation is very weak. However, this does not take into account the abilities of each of the athletes. another approach would be to fit a linear regression to each athlete's performance across the years.

```{r}
name_labs <- rep( nflfg_yeardif$Name, each = 3 )
BothY$Names <- name_labs

ggplot( BothY, aes( x = val1, y = val2, fill = Names ) ) +
  geom_point( shape = 21, alpha = 0.5, size = 4 ) +
  labs( title = 'Current by Previous Year' ) +
  xlab( 'Field Goal Percentage in Year t-1' ) +
  ylab( 'Field Goal Percentage in Year t' )
```
Now to look at the linear regression coefficients by athlete:

```{r}
by_athlete <- BothY %>%
  group_by( Names )

lm_byAthlete <- do( by_athlete, tidy( lm( val2 ~ val1, data = . ) ) )

lm_byAthlete <- lm_byAthlete %>%
  dplyr::select( c( Names, term, estimate ) ) %>%
  pivot_wider( names_from = term, values_from = estimate )


lm_byAthlete
```
Allowing for a different intercept for each athlete (different abilities), it can be shown that if a kicker had a high field goal percentage the previous year, then they are predicted to have a lower goal percentage the current year.


**2) Newspaper Circulation**
demonstrate the use of dummy variables along with transformations to overcome skewness

```{r}
circ_url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA621/master/circulation.csv'
circ_df <- read.table( circ_url, sep = '\t', header = TRUE )
circ_df <- rename( circ_df, Tabloid_hasComp = Tabloid.with.a.Serious.Competitor )
glimpse( circ_df )
```
The feature `Tabloid.with.a.Serious.Competitor` is a **dummy variable** it only takes a true/false value to indicate an outcome.

Take a look at Sunday ~ Weekday circulation grouped by the dummy variable: 
```{r}
ggplot( circ_df, aes( x = Weekday, y = Sunday, color = factor( Tabloid_hasComp ) ) ) +
  geom_point( size = 3)
```
We can make the variable much more constant (linear) by plotting the log values:

```{r}
ggplot( circ_df, aes( x = log(Weekday), y = log(Sunday), color = factor( Tabloid_hasComp ) ) ) +
  geom_point( size = 3)
```


**3) Menu Pricing in an NYC Restaurant**
highlights the use of multiple regression. this is a classic dataset: pricing East/West of 5th Ave.  
Produce a regression model to predict the price of dinner

```{r}
nyc_csv <- read.csv( 'nyc.csv' )
nyc_df <- data.frame( nyc_csv )
glimpse( nyc_df )
```
```{r}
dta <- nyc_df %>%
  dplyr::select( c( Price, Food, Decor, Service ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
  
```

check out the distributions of the East/West variable:
```{r}
ggplot( nyc_df, aes( x = factor( East ) , y = Price ) ) +
  geom_boxplot() +
  xlab( 'Direction off 5th Ave)' ) +
  ggtitle( 'Price ~ East vs West' ) +
  scale_x_discrete( labels = c( 'West', 'East' ) )
```

```{r}
mod <- lm( Price ~ Service + Decor + Food + East, nyc_df )
summary( mod )
```

**4) Wine Critics' Ratings**
```{r}
wine_csv <- read.csv( 'Bordeaux.csv' )
wine_df <- data.frame( wine_csv )
glimpse( wine_df )
```

```{r}
dta <- wine_df %>%
  dplyr::select( c( Price, ParkerPoints, CoatesPoints ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of features" )
  
```

```{r}
wine_df <- wine_df %>%
  mutate_at( vars( P95andAbove, FirstGrowth, 
                   CultWine, Pomerol, VintageSuperstar ),
             funs( factor ) )

firstG <- ggplot( wine_df, aes( y = Price, x = FirstGrowth ) ) +
  geom_boxplot()
p95 <- ggplot( wine_df, aes( y = Price, x = P95andAbove ) ) +
  geom_boxplot()
cult <- ggplot( wine_df, aes( y = Price, x = CultWine ) ) +
  geom_boxplot()
pom <- ggplot( wine_df, aes( y = Price, x = Pomerol ) ) +
  geom_boxplot()
VS <- ggplot( wine_df, aes( y = Price, x = VintageSuperstar ) ) +
  geom_boxplot()

grid.arrange( firstG, p95, cult, pom, VS, ncol = 3 )
```

```{r}
dta <- wine_df %>%
  dplyr::select( c( Price, ParkerPoints, CoatesPoints ) ) %>%
  mutate_at( vars( Price, ParkerPoints, CoatesPoints ),
             funs( log ) )
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Matrix plot of log(features)" )
  
```

Over the chapters in this book, will dive in to these data sets much deeper to model various aspects and build predictive capabilities



## Simple Linear Regression
modeling the relationship between two variables as a straight line, that is, when $Y$ is modeled as a linear function of $X$.

### Introduction to Least Squares Estimates

```{r}
production_url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA621/master/production.txt'
production_df <- read.table( production_url, sep = '\t', header = TRUE )
glimpse( production_df )
```

```{r}
ggplot( production_df, aes( x = RunSize, y = RunTime ) ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 4 ) +
  xlab( 'Run Size' ) +
  ylab( 'Run Time' )
```
 We wish to develop an equation to model the relationship between Y, the run time, and X, the run size.  
 
If the regression of Y on X is linear:
$$Y_i = \mbox{E}(Y|X=x) + e_i = \beta_0 + \beta_1 x + e_i$$
where $e_i$ is the random error in $Y_i$  
all unexplained variation is called random error. Random error does not depend on X, nor does it contain any information about Y (otherwise it would be systematic error).  
**residuals**: the difference between the actual value of y and the predictor value of y.  
we wish to describe a line of best fit which minimizes the residuals.  

fit the production data with a linear model and visualize the residuals:  
```{r}
mod <- lm( RunTime ~ RunSize, production_df )
production_df$pred <- predict( mod )
production_df$res <- residuals( mod )
glimpse( production_df )
```
```{r}
ggplot( production_df, aes( x = RunSize, y = RunTime ) ) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = RunSize, yend = pred), alpha = .2) + 
  geom_point() +
  geom_point(aes(y = pred), shape = 1) +
  theme_classic()  
```
**Least Squares** line of best fit
coefficients for the line of best fit are chosen so as to minimize the sum of the squared residuals:
$$\mbox{RSS} = \sum^n_{i=1}\hat{e}^2_i = \sum^n_{i=1}(y_y-\hat{y}_i)^2 = \sum^n_{i=1}(y_i - b_0 - b_1x_i)^2$$
```{r}
summary( mod )
```
The equation for the best fit line is given by:
$$y = \beta_0 + \beta_1 x = 149.7 + 0.26x$$
* The **intercept**, 149.7, is where the line of best fit crosses the y-axis.
* The **slope**, 0.26, estimates the change in relationship of `RunTime` as a function of `RunSize`. The slope is informative and describes a 0.26 increase in `RunTime` for every unit increase of `RunSize`.

Estimating the variance of the error $\sigma^2 = Var(e)$  
The residuals can be used to estimate $\sigma^2$
$$S^2 = \frac{\mbox{RSS}}{n-2} = \frac{1}{n-2}\sum^n_{i=1}\hat{e}_i^2$$
 
 
### Exercise 2.1
```{r}
playbill_df <- read.csv( 'playbill.csv' )
glimpse( playbill_df )
```
fit a linear model to describe the relationship CurrentWeek ~ LastWeek:

```{r}
playbill_mod <- lm( CurrentWeek ~ LastWeek, playbill_df )
playbill_mod_sum <- summary( playbill_mod )
playbill_mod_sum
```
visualize the results:
```{r}
ggplot( playbill_df, aes( x = LastWeek, y = CurrentWeek ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  xlab( 'Gross Box Office Results Last Week' ) +
  ylab( 'Gross Box Office Results Current Week' ) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  theme_classic() 
```

#### a) find the 95% confidence interval for the slope of the regression model. Is 1 a plausible value for the slope? Give a reason to support your answer

The 95% confidence interval is given by the estimated slope coefficient $\pm$ 2*SE:
```{r}
#can be calculated directly from the coefficients
slope_coeff <- playbill_mod_sum$coefficients[ 2,1 ]
slope_SE <- playbill_mod_sum$coefficients[ 2,2 ]
slope_CI <- c( slope_coeff - 2*slope_SE, slope_coeff + 2*slope_SE )
slope_CI

#alternatively can use the function confint
confint( playbill_mod, 'LastWeek', level = 0.95 )
```
 We can say with 95% confidence that slope of the regression line is within the range of `r slope_CI[ 1 ]` and `r slope_CI[ 2 ]`
 Given this range, we can conclude that 1 is a plausible value for the slope of the line as it is within the CI.
 
 #### b) Test the null hypothesis that $H_0: \beta_0 = 10000$ against a two-sided alternative. Interpret your results.
 taking a look at the confidence interval for intercept:
```{r}
confint( playbill_mod, '(Intercept)', level = 0.95 )
```
10,000 is well withing the 95% confidence interval. Therefore we fail to reject the null hypothesis that $\beta_0 = 10000$

#### c) Use the fitted regression model to estimate the gross box office results for the current week (in $) for a production with $400,000 in gross box office the previous week. Find a 95% prediction interval for the gross box office results for the current week (in $) for a production with $400,000 in gross box office the previous week. Is $450,000 a feasible value for the gross box office results in the current week, for a production with $400,000 in gross box office the previous week? Give a reason to support your answer.

```{r}
lw400k <- data.frame( 'LastWeek' = 400000 )
pred <- predict( playbill_mod, newdata = lw400k, interval = 'predict' )
res <- paste( '1) Estimated GBO w/ LastWeek(400000):', round( pred[ 1 ], 0 ), 
              '\n2) 95% Prediction Interval: Lwr =', round( pred[ 2 ], 0 ), 
              'Upr =', round( pred[ 3 ], 0 ) )
cat( res, sep = '\n' )
```
3) from the prediction interval calculated above, we see that $450,000 is not feasible, because it lies well above the upper limit of the prediction interval (Upr = `r round( pred[ 3 ], 0 )`)
 
#### d) Some promoters of Broadway plays use the prediction rule that next week's gross box office results will be equal to this week's gross box office results. Comment on the appropriateness of this rule. 

The assertion is approximately correct. The slope of the regression line that models a current week's BO gross result is approximately 1 (slightly less at slope = `r playbill_mod_sum$coefficients[ 2,1 ]`) and 1.0 is well with in the confidence interval.

### Exercise 2.2
A story by James R. Hagerty entitled 'With Buyers Sidelined, Home Prices Slide' published in the Wall Street Journal contained data on so-called fundamental housing indicators in major real estate markets across the US. The author argues that, 'prices are generally falling and loan payments are piling up.' Thus, we shall consider data presented in the article.

loading the data:
```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/indicators.txt'
indicators_df <- read.csv( path, sep = '\t', header = TRUE )
glimpse( indicators_df )
```

Fit the following model: $Y = \beta_0 + \beta_1x + \epsilon$  
Where:  

* Y = Percent change in average price 
* x = Percent of mortgage loans 30 days or more overdue
```{r}
indicators_mod <- lm( PriceChange ~ LoanPaymentsOverdue, data = indicators_df )
indicators_mod_sum <- summary( indicators_mod )
indicators_mod_sum
```
visualize the data as well:
```{r}
ggplot( indicators_df, aes( x = LoanPaymentsOverdue, y = PriceChange ) ) +
  geom_point(fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  xlab( 'Percentage of Mortgage Loans >= 30days Overdue' ) +
  ylab( 'Percent Change in Average Price' ) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  theme_classic() 
```

#### a) Find a 95% confidence interval for the slope of the regression model
```{r}
confint( indicators_mod, 'LoanPaymentsOverdue', level = 0.95 )
```
judging by the range of the 95% confidence intervals, a negative linear association between the Percent change in average price by the percentage of overdue mortgage loans is not statistically significant because the 95% CI range extents into positive slope values.

#### b) use the fitted regression model to estimate $E(Y|X=4)$.
Find a 95% CI for $E(Y|X=4)$. Is 0% a feasible value for $E(Y|X=4)$? Give a reason to support your answer.

use the model to predict the expected value at X = 4:
```{r}
exp4 <- data.frame( 'LoanPaymentsOverdue' = 4 )
pred <- predict( indicators_mod, newdata = exp4, interval = 'predict' )
res <- paste( '1) E(Y|X=4):', round( pred[ 1 ], 0 ), 
              '\n2) 95% Prediction Interval: Lwr =', round( pred[ 2 ], 0 ), 
              'Upr =', round( pred[ 3 ], 0 ) )
cat( res, sep = '\n' )
```
A value of 0% is well within the CI for the expected value of Y given X=4. Therefore, with only this data, there is no statistical significance to support a negative relationship between changes in housing prices and percentages of overdue loans.
 
### Inferences about the Slope and Intercept
find confidence intervals for hypothesis tests about either the slope of the intercept of a regression line.  

We need to make some assumptions:  

* The relationship between the response and predictor is linear
* the errors are independent
* the errors are normally distributed.

intercept
$$\hat{\beta}_0 = y - \hat{\beta}_1\bar{x}$$
slope
$$\hat{\beta}_1 = \frac{SXY}{SXX}$$
where:  
* **SXY**: the sum of the product of the difference of xs and the mean and the difference of ys and their means
* **SXX**: the sum of squares of the differences between each x and their mean

we can also write that
$$\hat{\beta}_1 = \frac{x_i-\bar{x}}{SXX}$$
from this emerge 3 useful relations:

1)
$$E(\hat{\beta}_1|X) = \hat{\beta}_1$$

2)
$$Var(\hat{\beta}_1|X) = \frac{\sigma^2}{SXX}$$

3)
$$\hat{\beta}_1|X \sim N \left( \hat{\beta}_1, \frac{\sigma^1}{SXX} \right)$$
 
find a 95% Confidence Interval for the production example:
```{r}
glimpse( production_df )
summary( mod )
```
```{r}
confint( mod )
```
 can find manually:
```{r}
#for the slope
#from the summary( mod ) output
modsum <- summary( mod )
slope <- modsum$coefficients[ 2,1 ]
SE_slope <- modsum$coefficients[ 2,2 ]
n <- nrow( production_df )
tdist<- qt( 0.025, n-2 )
CI <- c( slope - tdist*SE_slope, slope + tdist*SE_slope )
CI
```
 
```{r}
#for the intercept
int <- modsum$coefficients[ 1,1 ]
SE_int <- modsum$coefficients[ 1,2 ]
n <- nrow( production_df )
tdist<- qt( 0.025, n-2 )
CI <- c( int - tdist*SE_int, int + tdist*SE_int )
CI
```

### Confidence Intervals for the Population Regression Line
$$E(Y|X = x^*) = \hat{\beta}_0 + \hat{\beta}_1x^* = y^*$$
 this leads to 
 $$Var(\hat{y}^*) = \sigma^2 \left( \frac{1}{n} + \frac{x^* - \bar{x}}{SXX} \right)$$

### Prediction Intervals for the Actual Value of Y
finding a prediction interval for the actual value of Y at $x^*$, a given value of X.  

* $E(Y|X = x^*)$ the expected value or average value of Y for a given value $x^*$: this is what you expect, a fixed mean, that lies on the regression line. $E(Y|X = x^*) \neq Y$ because Y can take many values
* $E(Y|X = x^*) \neq Y^*$. because $Y^*$ can be lots of values and is probably not on the regression line at all.
* Confidence Intervals are always reported for a parameter. Prediction Intervals are reported for random values ($Y^*$)

Confidence Intervals $\neq$ Prediction Intervals: The variability in the error of predicting a single value of Y will exceed the variability for estimating the expected value of Y, because the prediction must account for the random error $e^*$ of the system.

```{r}
# compare these CI values.....
head( predict( mod, interval = 'confidence' ) )
```

```{r}
# ....with these PI values:
head( predict( mod, interval = 'prediction' ) )
```
We see that the estimates are the same. However, the interval span is much larger for the prediction than the confidence intervals.

### ANOVA
To test whether there is a linear association between Y and X, we have to test $H_0 : \beta_1 = 0 \mbox{  against  } H_A: \beta_1 \neq 0$. This can be evaluated with F-test statistic.
$$F = \frac{\frac{SSreg}{1}}{\frac{RSS}{(n-2)}}$$
where **SSreg**: sum of squares explained by the regression model (sum of squares of difference of $\hat{y}-\bar{y}$ (the regression est - mean)) 
**RSS**: residual sum of squares (sum of squares of differences of the observed y and the regression models est $\hat{y}$)

### Dummy Variable Regression
testing the means of two groups

```{r}
changeover_df <- read.csv( '/home/bonzilla/Documents/MSDS/DATA621/changeover.txt', sep = '\t', header = TRUE )
changeover_df <- changeover_df %>%
  mutate( Method = as.factor( Method ),
          New = as.factor( New ) )
glimpse( changeover_df )
```
```{r}
ggplot( changeover_df, aes( x = Method, y = Changeover ) ) +
  geom_boxplot() +
  theme_classic() 
```
We wish to develop an equation to model the relationship between Changeover and the Method. Does the new method decrease the changeover time?

```{r}
co_mod <- lm( Changeover ~ Method, data = changeover_df )
summary( co_mod )
```
We wish to test whether the coefficient of the dummy variable is significantly different than 0.

From the model summary output, we see that the t-value is -2.254 and that the p-value is significant at $\alpha = 0.05$. Therefore, there is significant evidence of a reduction in means for the new method.

Find the confidence interval:
```{r}
confint( co_mod)
```
Note that CI for the New method does not span 0; this means that we can reject the null hypothesis that there is no difference between the means of the old and new methods.

### Exercise 2.3
The manager of the purchasing department of a large company would like to develop a regression model to predict the average amount of time it takes to process a given number of invoices. Over a 30 day period, data are collected on the number of invoices processed and the total time taken (in hours). The following model was fit to the data: $\mbox{processing time} = \beta_0 + \beta_1 \mbox{#invoices} + \epsilon$ 

```{r}
invoices_df <- read.csv( '/home/bonzilla/Documents/MSDS/DATA621/invoices.txt', sep = '\t', header = TRUE )
glimpse( invoices_df ) 
ggplot( invoices_df, aes( x = Invoices, y = Time ) ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  theme_classic() +
  ggtitle( 'Scatterplot of Invoices data' )
```


a) Find a 95% confidence interval for the start-up time, i.e. $\beta_0$

```{r}
invoices_mod <- lm( Time ~ Invoices, data = invoices_df )
invoices_sum <- summary( invoices_mod )
invoices_sum
confint( invoices_mod )
```


b) Suppose that a best practice benchmark for the average processing time for an additional invoice is 0.01 hours (or 0.6 minutes). Test the null hypothesis against the two-sided alternative. Interpret your result.

The confidence interval for the Invoices coefficient spans the value $\beta_1 = 0.01$. Therefore, we cannot reject the null hypothesis that the relationship is any different that the benchmark

c) Find a point estimate and a 95% prediction interval for the time taken to process 130 invoices.

```{r}
predict( invoices_mod, newdata = data.frame( 'Invoices' = 130 ), interval = 'confidence' )
```

## Diagnostics and Transformations for Simple Linear Regression

Numerical regression output should always be supplemented by an analysis to ensure that an appropriate model has been fitted to the data.  

**Residuals**: using plots of residuals to determine whether the proposed regression model is a valid model. If a linear model is appropriate, the residuals should resemble random errors; there should be no pattern. If the residuals vary with respect to x, then this indicates that an incorrect model has been fit.

### Regression Diagnostics: Tools for checking the validity of a model.

* Is the model valid?: does the model fit the data. use plots of standardized residuals
* Are there any data points with unusually large leverage?
* Are any of the data points extreme outliers?
* If a leverage point exists, is it a bad one?
* Is the assumption of constant variance of the errors reasonable?
* If the data are collected over time, examine whether the data are correlated over time.
* Is the assumption that the errors are normally distributed reasonable?

#### Leverage Points
**Leverage Points**: data which exercise considerable influence on the fitted model

a look at Huber's good/bad leverage points
```{r}
x <- c( -4, -3, -2, -1, 0, 10 )
YBad <- c( 2.48, 0.73, -0.04, -1.44, -1.32, 0 )
YGood <- c( 2.48, 0.73, -0.04, -1.44, -1.32, -11.4)

huber <- data.frame( x, YBad, YGood ) %>%
  pivot_longer( cols = c( YBad, YGood ), names_to = 'Y', values_to = 'val' ) %>%
  ggplot( aes( x = x, y = val, color = Y  ) ) +
  geom_point() +
  geom_smooth( method = 'lm' )
huber  
```

In the above plot, x = 10 is a leverage point in both data series: the value of x is far from the other values and the value of Y has a very large effect on the least squares regression line.

Finding a numerical rule to identify leverage points based on (1) the distance of x is away from the bulk of the x's and (2) the extent to which the fitted regression line is attracted to a given point.  
$$\mbox{Leverage } \rightarrow h_{ii} = \frac{1}{n}+\frac{(x_i - \bar{x})^2}{\sum_{j=1}^n(x_j-\bar{x})^2}$$
**The Rule**: classify $x_i$ as a point of hight leverage in a simple linear regression model if:
$$h_{ii} \gt 2 \cdot average( h_{ii} ) = 2 \cdot \frac{2}{n} = \frac{4}{n}$$

Strategies for dealing with Bad leverage points:  

* Remove invalid data points
* Fit a different regression model

Standardize Residuals: divide the residuals by an estimate of the standard deviation
$$r_i = \frac{\hat{e}_i}{s \sqrt{1-h_{ii}}}$$
where s = $\sigma$  
**Common Practice**: label points as outliers in small/moderate datasets if the point falls outside the interval of -2 to 2 standard deviations out. Otherwise, for large datasets, use the range -4 to 4.  
*Identification and examination of any outliers is a key part of regression analysis*

**Bad Leverage pnt**: a leverage point whose standardized residual is outside -2 to 2  
**Good Leverage pnt**: a leverage point whose standardized residual is inside the interval -2 to 2  

a relatively small number of outlying points can have a relatively large effect on the fitted model. 
```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/bonds.csv'
bonds <- read.csv( path, sep = '\t' )
glimpse( bonds )
```

```{r}
ggplot( bonds, aes( x = CouponRate, y = BidPrice ) ) +
  geom_point() +
  geom_smooth( method = 'lm' )
```

The linear model doesn't do a great job here. However, this is largely because of the influence of three data points to the left.
```{r}
bonds_mod <- lm( BidPrice ~ CouponRate, bonds )
standard_res <- rstandard( bonds_mod )
bonds$standard_res <- standard_res

ggplot( bonds, aes( x = CouponRate, y = standard_res ) ) +
  geom_point() +
  geom_hline( yintercept = -2, color = 'blue' ) +
  geom_hline( yintercept = 2, color = 'blue' )
```
As it turns out, the 3 points to the left are 'flower' bonds, which can be justifiably treated differently from the rest of the data set.  
* Points should not be routinely deleted from an analysis just because they do not fit the model
* Outliers often point out an important feature of the problem not considered before.

**Cook's Distance**: multiply the quantities: square of the ith standardized residual * 2 x a monotonic function that increases as the ith leverage value increases
use the recommended cut-off value of $\frac{4}{n-2}$
```{r}
bonds_cooks <- cooks.distance( bonds_mod )
bonds$cooks <- bonds_cooks

ggplot( bonds, aes( x = CouponRate, y = cooks ) ) +
  geom_point() +
  geom_hline( yintercept = 4/(35-2), color = 'blue' )
```

#### Constant Variance
Does the errors/residuals have constant variance?  
Ignoring nonconstant variance when it exists invalidates all inferential tools (pvals, CIs etc)
```{r}
cleaning_df <- read.csv( '/home/bonzilla/Documents/MSDS/DATA621/cleaning.csv', sep = '\t', header = TRUE )
glimpse( cleaning_df )
```
```{r}
ggplot( cleaning_df, aes( x = Crews, y = Rooms ) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F ) +
  theme_classic()
```
```{r}
cleaning_mod <- lm( data = cleaning_df, Rooms ~ Crews )
aug_cleaning_mod <- augment( cleaning_mod )
p1 <- ggplot( aug_cleaning_mod, aes( x = Crews, y = .std.resid) ) +
  geom_point() +
  theme_classic()
p1
plot( cleaning_mod )
```
The standardized residuals x X does not have constant variance. Will see how to deal with this next section


### Transformations
How to use transformations to overcome problems due to non-constant variance and/or nonlinearity.

When both Y and X are measured in the same units then it is often natural to consider the same transformation for both X and Y.
```{r}
sqmod <- lm( data = cleaning_df, sqrt(Rooms) ~ sqrt(Crews) )
aug_sqmod <- augment( sqmod )
glimpse( aug_sqmod )
```
```{r}
p1 <- ggplot( aug_sqmod, aes( x = `sqrt(Crews)`, y = `sqrt(Rooms)`) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F ) +
  theme_classic()
p2 <- ggplot( aug_sqmod, aes( x = `sqrt(Crews)`, y = .std.resid) ) +
  geom_point() +
  theme_classic()

grid.arrange( p1, p2, ncol = 2 )
```

Note that the standardized residuals do not have the funnel shape they took on last section

```{r}
plot( sqmod )
```
 
#### Using Logarithms to estimate percentage effects

```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/confood1.csv'
confood1 <- read.csv( path, sep = '\t' )
glimpse( confood1 )
```
```{r}
ggplot( confood1, aes( x = Price, y = Sales ) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F) +
  theme_classic()
```

When studying the relationship between price and quantity in economics, it is common practice to take the logarithms of both since interest lies in predicting the effect of a 1% increase in price on quantity sold:
$$log(Q) = \beta_0 + \beta_1 log(P) + \epsilon$$
Here the slope of the model approximately equals the ratio of the percentage changes in Q & P. (Price Elasticity)
```{r}
ggplot( confood1, aes( x = log(Price), y = log(Sales ) ) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F) +
  theme_classic()
```

#### Using transformations to overcome problems due to nonlinearity:
Inverse response plots and Box-Cox procedures  

3 scenarios:  

* Response variable needs to be transformed

```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/restrans.csv'
restrans <- read.csv( path, sep = '\t' )
ggplot( restrans, aes( x = x, y = y ) ) +
  geom_point() +
  theme_classic()
restrans_mod <- lm( data = restrans, y ~ x )
plot( restrans_mod )
```

plainly obviously not a linear model candidate
```{r}
inverseResponsePlot( restrans_mod, lambda = c(0, 0.33, 1), 
    robust=FALSE, xlab=NULL, id=FALSE )
```

Transform a response variable using the Box-Cox method: the box-cox procedure aims to find the transform that makes the transformed variable close to normally distributed.  

```{r}
restrans_mod_trans <- lm( data = restrans, y^(1/3) ~ x )
restrans_mod_trans_aug <- augment( restrans_mod_trans )
glimpse( restrans_mod_trans_aug )

ggplot( restrans_mod_trans_aug, aes( x = x, y = `y^(1/3)`)) +
  geom_point() +
  geom_smooth( method = 'lm', se = F ) +
  theme_classic()
```

Check out how damn linear the dataseries is after the transform.  
Next visualize the diagnostics:
```{r}
plot( restrans_mod_trans )
```


* Predictor variable needs to be transformed
again, consider scaled power transformations.  
Also, considering only transforming only X, so fitting models of the form:  
$E(Y|X=x) = \alpha_0 + ]\alpha_1 \psi_S(x,\lambda)$


* both response and predictor need to be transformed
Multivariate generalization of the Box-Cox transformation method

How To Think About It:
![](/home/bonzilla/Documents/MSDS/DATA621/logicflow.png)

## Multiple Linear Regression
It is common for more than one factor to influence an outcome.  
Checking Multiple Linear Regression Model Validity...

### Polynomial Regression
Modeling Salary from years of experience
```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/profsalary.csv'
profsalary <- read.csv( path, sep = '\t' )
glimpse( profsalary )
p1 <- ggplot( profsalary, aes( x = Experience, y = Salary ) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F ) +
  theme_classic()
p1
```
very clearly a linear model is inappropriate here.

```{r}
quadmod <- lm( Salary ~ Experience + I( Experience^2 ), data = profsalary )
quadmodaug <- augment( quadmod )
glimpse( quadmodaug )
```
```{r}
p2 <- p1 + geom_line( data = quadmodaug, aes( x = Experience, y = .fitted ), color = 'red' )
p3 <- ggplot( quadmodaug, aes( x = Experience, y = .std.resid ) ) +
  geom_point() +
  theme_classic()
p4 <- ggplot( quadmodaug, aes( x = Experience, y = .cooksd ) ) +
  geom_point() +
  theme_classic()
grid.arrange( p2, p3, p4, ncol = 2 )
```

go ahead and look at the diagnostics:
```{r}
plot( quadmod )
```

### Estimation and Inference in Multiple Linear Regression
Multiple Linear Regression:
$$E(Y|X-1=x_1,X_2=x_2,\dots , X_p=x_p) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + beta_px_p$$
Thus,
$$Y_1 = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + beta_px_p +\epsilon_i$$
**Degrees of Freedom** = Sample Size - Number of mean parameters  

For linear regression, $R^2 = \frac{SSreg}{SST} = 1 - \frac{RSS}{SST} $  
However, adding irrelevant predictor variables to the regression equation often increases $R^2$. To compensate, we define an adjusted coefficient of determination: $R^2_{adj} = 1 - \frac{RSS/(n-p-1)}{SST/(n-1)}$

The F-test is also used to test the linear association between Y and ANY of the p x-varaibles. If the F-test is significant then the next natural question to ask is which of the p x-variables is there evidence of a linear association.

Testing whether a subset of predictors have regression coefficients = 0:
Ex: Menu pricing at a new Italian restaurant in NYC
```{r}
nyc_csv <- read.csv( 'nyc.csv' )
nyc_df <- data.frame( nyc_csv )
glimpse( nyc_df )
```
```{r}
nyc_mod <- lm( data = nyc_df, Price ~ Food + Decor + Service + East )
summary( nyc_mod )
```

`Decor` has the largest reg. corefficient and is the most statistically significant to the model whereas `Service`'s coefficient does not appear to be statistically significant and has the smallest magnitude.

### Analysis of Covariance
Possible outcomes:  

* coincident regression lines
* parallel regression line
* regression lines with equal intercepts but different slopes
* unrelated regression line

Ex: Amount spent on travel
```{r}
path <- '/home/bonzilla/Documents/MSDS/DATA621/travel.csv'
travel <- read.csv( path, sep = '\t' )
glimpse( travel )
```
```{r}
ggplot( travel, aes( x = Age, y = Amount, color = factor(Segment) ) ) +
  geom_point() +
  theme_classic()
```
```{r}
travel_mod <- lm( data = travel, Amount ~ Age + C + C*Age )
summary( travel_mod )
```
Reduced Model:
```{r}
travel_mod_reduced <- lm( data = travel, Amount ~ Age )
summary( travel_mod_reduced )
```
perform ANOVA to look at the F-test & compare the two models:
```{r}
anova( travel_mod_reduced, travel_mod )
```
There is very strong evidence against the reduced model. Thus, we prefer the unrelated regression lines model (full) to the coincident lines model (reduced)

Going back to the NYC Italian restaurants:
```{r}
nyc_full_mod <- lm( Price ~ Food + Decor + Service + East + Food:East + Decor:East + Service:East, nyc_df )
summary( nyc_full_mod )
```

NYC 'final' mod:
```{r}
nyc_final_mod <- lm( Price ~ Food + Decor + East, nyc_df )
summary( nyc_full_mod )
```

now to look at the F-test for the ANOVA:
```{r}
anova( nyc_full_mod, nyc_final_mod )
```
Given the ANOVA p-val, there is little evidence to reject the null hypothesis (no difference between models). Therefore, we can adopt the simpler reduced model without losing much predictive power.

## Diagnostics and Transformations for Multiple Linear Regression

## Variable Selection

## Logistic Regression

**Logistic Regression** - here we consider the situation where the response variable is a binary yes/no response. Ideally such responses follow a binomial distribution in which case the appropriate model is a logistic regression model.  

### Logistic Regression based on a single predictor  

**Binomial Distributions** - $Y \sim \mbox{Bin}(m,\theta)$ a process where:  

* there are $m$ identical trials
* each trial results in one of two outcomes, either a success or a failure
* the probability for a success is the same for all trials
* the trials are independent

Binomial process trials are caller **Bernoulli trials**  
consider the sample proportions of 'successes', $\frac{y_i}{m_i}$ as an unbiased estimate of $\theta$ that varies between 0 and 1.  
Least squares regression is inappropriate for binomial processes because the variance of the response is not constant

```{r}
zagatNYC <- read.csv( 'MichelinFood.csv', sep = '\t' )
glimpse( zagatNYC )
```
```{r}
ggplot( aes( y  = proportion, x = Food ), data = zagatNYC ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  theme_classic() +
  ggtitle( 'Sample Proportion ~ Food Rating' )
```
The envelope of the data makes it clear that a straight linear fit does not adequately describe the data. Rather, a sigmoidal curve gives the best approximation.  

### The Logistic Function and Odds.

A popular choice for the sigmoidal function is the logistic function:
$$\theta (x) = \frac{exp(\beta_0 + \beta_1x)}{1+exp(\beta_0 + \beta_1x} = \frac{1}{1+exp(-(\beta_0 + \beta_1x)}$$
$$\beta_0 + \beta_1x = log \left(\frac{\theta(x)}{1 - \theta(x)} \right)$$
$$\mbox{logit} = \frac{\theta(x)}{1 - \theta(x)}$$

if $\theta = P(\mbox{success})$, 
$$\mbox{Odds in favor of success} = \frac{P(\mbox{success})}{1-P(\mbox{success})} = \frac{\theta}{1-\theta}$$
$$\mbox{Odds against success} = \frac{1-P(\mbox{success})}{P(\mbox{success})} = \frac{1-\theta}{\theta}$$

```{r}
ggplot( aes( y  = proportion, x = Food ), data = zagatNYC ) +
  geom_point( fill = NA, shape = 21, alpha = 0.5, size = 3 ) +
  geom_smooth( method = 'glm', col = 'red', method.args = list( family = 'binomial'), se = F ) +
  theme_classic() +
  ylab( 'P(included in Michelin)' ) +
  xlab( 'Zagat Food Rating' ) +
  ggtitle( 'Probability of Inclusion in Michelin Guide ~ Food Rating' )
```

```{r}
zagatNYC_mod <- glm( cbind( InMichelin, NotInMichelin ) ~ Food, family = binomial, zagatNYC )
summary( zagatNYC_mod )
```

From this result, if the Zagat food rating is increased by one unit, then the odds of being included in the Michelin guide increases by `r exp(zagatNYC_mod$coefficients[2] )`


Find the estimated probabilities and the odds obtained using the logistic model
```{r}
zagatNYC_odds <- data.frame( 'Food' = zagatNYC$Food, 'pInc' = fitted( zagatNYC_mod ) ) %>%
  mutate( odds = pInc / ( 1 - pInc ) )
head( zagatNYC_odds )
```


#### Likelihood for Logistic Regression with a Single Predictor
How likelihood can be used to estimate the parameters of logistic regression  
The likelihood function is the function of the unknown probability of success

$$log(1 - \theta(x_i)) = log \left( \frac{1}{1+exp(-(\beta_0 + \beta_1x)} \right)$$
and the parameters $\beta_0 \mbox{ & } \beta_1$ can be estimated by maximizing the log-likelihood canbe found with the Newton-Raphson method.  
testing the hypothesis $H_0 : \beta_1 = 0$ is to use the Wald test statistic

#### Explanation of Deviance
**deviance** replaces the concept of residual sum of squares.  
$$G^2 = 2\sum_{i=1}^n \left[ y_i\mbox{log} \left(\frac{y_i}{\hat{y_i}}\right) + (m_i-y_i)\mbox{log}\left(\frac{m_i-y_i}{m_i-\hat{y_i}}\right) \right]$$
We wish to test:  

* $H_0$ - the logistic regression model is appropriate
* $H_A$ - the logistic regression model is inappropriate

```{r}
zagatNYC_mod_sum <- summary( zagatNYC_mod )
zagatNYC_mod_sum
```


#### Using differences in Deviance Values to Compare Models 

Compare the null  and residual deviances to test $H_0 : \beta_1 = 0$ against $H_A : \beta_1 \ne 0$
```{r}
zagatNYC_mod_anova <- anova( zagatNYC_mod, test = 'F' )
zagatNYC_mod_anova
```
 

#### $R^2$ for Logistic Regression

$$R^2_{dev} =  1 - \frac{G^2_{H_A}}{G^2_{H_0}}$$
```{r}
zagatNYC_mod_anova$`Resid. Dev`
```
```{r}
R2dev <- 1 - zagatNYC_mod_anova$`Resid. Dev`[2]/zagatNYC_mod_anova$`Resid. Dev`[1]
R2dev
```
can also look at the Pearson goodness-of-fit stat to evaluate

#### Residuals for Logistic Regression
diagnostic procedures for logistic regression.  

Can look at :

* Response residuals - can be difficult to interpret because of nonconstant variance
* standardized Pearson's residuals
* standardized Deviance residuals

```{r}
zagatNYC_3res <- data.frame( 'Food' = zagatNYC$Food, 'Response' = zagatNYC$proportion ) %>%
  mutate( Res_resid = zagatNYC_mod$residuals,
    dev_resid = residuals( zagatNYC_mod, type = 'response' ),
    pear_resid = residuals( zagatNYC_mod, type = 'pearson' ) )
head( zagatNYC_3res )
```


### Binary Logistic Regression 

the binary form of the zagat data:
```{r}
zagatBinary <- read.csv( '/home/bonzilla/Documents/MSDS/DATA621/MichelinNY.csv' )
glimpse( zagatBinary )
```
```{r}
ggplot( aes( y  = InMichelin, x = Food, col = factor(InMichelin) ), data = zagatBinary ) +
  geom_jitter( fill = NA, shape = 21, alpha = 0.5, size = 3, width = 0.1, height = 0.1 ) +
  geom_smooth( method = 'glm', col = 'red', method.args = list( family = 'binomial'), se = F ) +
  theme_classic() +
  ylab( 'Included in Michelin?' ) +
  xlab( 'Zagat Food Rating' ) +
  ggtitle( 'In Michelin Guide ~ Food Rating' )
```

```{r}
zagatbinary_mod <- glm( InMichelin ~ Food, family = binomial, zagatBinary )
summary( zagatbinary_mod )
```
Notice that the coefficients are the same, but the AIC and deviance are completely different


#### Deviance for the case of Binary Data
The distribution of the difference in deviances is approximately $\chi^2$

#### Residuals for Binary Data  
```{r}
zagatBinary_res <- data.frame( 'Food' = zagatBinary$Food ) %>%
  mutate( stPearson = resid(zagatbinary_mod, type='pearson')/sqrt(1 - hatvalues(zagatbinary_mod)),
          stDeviance = rstandard( zagatbinary_mod ) )

p1 <- ggplot( data = zagatBinary_res, aes( x = stDeviance, y = Food ) ) +  
  geom_point( fill = NA, shape = 21, alpha = 0.3, size = 3 ) +
  theme_classic() +
  ylab( 'Food Rating' ) +
  ggtitle( 'Standardized Deviance Residuals' )
p2 <- ggplot( data = zagatBinary_res, aes( x = stPearson, y = Food ) ) +  
  geom_point( fill = NA, shape = 21, alpha = 0.3, size = 3 ) +
  theme_classic() +
  ylab( 'Food Rating' ) +
  ggtitle( 'Standardized Pearson Residuals' )
  
grid.arrange( p1, p2, ncol = 2 )
```
There are some similar and highly random patterns here. Residual plots are very problematic for binary data!

#### Transforming Predictors in Logistic Regression for Binary Data.
When the predictor variable $X$ is normally distributed with a different variance for the two values of $Y$, the log odds are a quadratic function of $x$  
When the predictor variable $X$ is normally distributed with the same variance for the two values of $Y$, the log odds are a linear function of $x$

Transforming skewed predictor variables....
```{r}
p1 <- ggplot( data = zagatBinary, aes( x = factor( InMichelin ), y = Food ) ) +
  geom_boxplot() +
  theme_classic()
p2 <- ggplot( data = zagatBinary, aes( x = factor( InMichelin ), y = Decor  ) ) +
  geom_boxplot() +
  theme_classic()
p3 <- ggplot( data = zagatBinary, aes( x = factor( InMichelin ), y = Service ) ) +
  geom_boxplot() +
  theme_classic()
p4 <- ggplot( data = zagatBinary, aes( x = factor( InMichelin ), y = Price ) ) +
  geom_boxplot() +
  theme_classic()

grid.arrange( p1, p2, p3, p4, ncol = 2 )
```
Let's consider the following logistic regression model with these 4 predictors:
$$\theta(x)=\frac{0}{1+ exp(-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_4x_4 + \beta_5log(x_4))}$$


#### Marginal Model plots for Binary Data
residual plots are difficult to interpret, but marginal model plots are an alternative...

```{r message=FALSE}
#glimpse( zagatBinary )
zagatbinary_fullmod <- glm( InMichelin ~ Food + Decor + Service + Price + log(Price), family = binomial, zagatBinary )
mmps( zagatbinary_fullmod )
```

The plots of Service and Decor don't track as well with the data as the other features. Here we add a term to the model to look at interaction effects between the two variables:

```{r}
zagatbinary_fullmod2 <- glm( InMichelin ~ Food + Decor + Service + Price + log( Price ) + Service:Decor, family = binomial, zagatBinary )
mmps( zagatbinary_fullmod2 )
```

```{r}
anova( zagatbinary_fullmod, zagatbinary_fullmod2 )
```
Look at the summary for our new model:
```{r}
summary( zagatbinary_fullmod2 )
```

Price is not very significant here, so lets simplify the model a bit:
```{r}
zagatbinary_fullmod3 <- glm( InMichelin ~ Food + Decor + Service + log( Price ) + Service:Decor, family = binomial, zagatBinary )
summary( zagatbinary_fullmod3 )
mmps( zagatbinary_fullmod3 )
```
The marginal plots show reasonable agreement and all the model terms are significant at least up to $\alpha$ = 0.05.


####





<br><br><br>